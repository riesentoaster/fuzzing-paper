@article{UNIX,
  author     = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title      = {An Empirical Study of the Reliability of UNIX Utilities},
  year       = {1990},
  issue_date = {Dec. 1990},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/96267.96279},
  doi        = {10.1145/96267.96279},
  abstract   = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  journal    = {Commun. ACM},
  month      = {dec},
  pages      = {32–44},
  numpages   = {13}
}

@inproceedings{DART,
  author    = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  title     = {DART: Directed Automated Random Testing},
  year      = {2005},
  isbn      = {1595930566},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1065010.1065036},
  doi       = {10.1145/1065010.1065036},
  abstract  = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {213–223},
  numpages  = {11},
  keywords  = {software testing, interfaces, random testing, automated test generation, program verification},
  location  = {Chicago, IL, USA},
  series    = {PLDI '05}
}

@inproceedings{SAGE,
  title     = {Automated Whitebox Fuzz Testing},
  author    = {Patrice Godefroid and Michael Y. Levin and David A. Molnar},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2008},
  url       = {https://api.semanticscholar.org/CorpusID:1296783}
}

@inproceedings{GrammarBasedWhiteboxFuzzing,
  author    = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
  title     = {Grammar-Based Whitebox Fuzzing},
  year      = {2008},
  isbn      = {9781595938602},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1375581.1375607},
  doi       = {10.1145/1375581.1375607},
  abstract  = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages.In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {206–215},
  numpages  = {10},
  keywords  = {grammars, automatic test generation, software testing, program verification},
  location  = {Tucson, AZ, USA},
  series    = {PLDI '08}
}

@inproceedings{KLEE,
  author    = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  title     = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
  year      = {2008},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {209–224},
  numpages  = {16},
  location  = {San Diego, California},
  series    = {OSDI'08}
}

@article{Science,
  title    = {A systematic review of fuzzing techniques},
  journal  = {Computers \& Security},
  volume   = {75},
  pages    = {118-137},
  year     = {2018},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2018.02.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404818300658},
  author   = {Chen Chen and Baojiang Cui and Jinxin Ma and Runpu Wu and Jianchao Guo and Wenqian Liu},
  keywords = {Software bug, Vulnerability, Fuzzing, Dynamic symbolic execution, Coverage guide, Grammar representation, Scheduling algorithms, Taint analysis, Static analysis},
  abstract = {Fuzzing is an effective and widely used technique for finding security bugs and vulnerabilities in software. It inputs irregular test data into a target program to try to trigger a vulnerable condition in the program execution. Since the first random fuzzing system was constructed, fuzzing efficiency has been greatly improved by combination with several useful techniques, including dynamic symbolic execution, coverage guide, grammar representation, scheduling algorithms, dynamic taint analysis, static analysis and machine learning. In this paper, we will systematically review these techniques and their corresponding representative fuzzing systems. By introducing the principles, advantages and disadvantages of these techniques, we hope to provide researchers with a systematic and deeper understanding of fuzzing techniques and provide some references for this field.}
}

@inproceedings{Driller,
  title     = {Driller: Augmenting Fuzzing Through Selective Symbolic Execution},
  author    = {Nick Stephens and John Grosen and Christopher Salls and Andrew Dutcher and Ruoyu Wang and Jacopo Corbetta and Yan Shoshitaishvili and Christopher Kr{\"u}gel and Giovanni Vigna},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2016},
  url       = {https://api.semanticscholar.org/CorpusID:2388545}
}

@inproceedings{AFLPlusPlus,
  author    = {Andrea Fioraldi and Dominik Maier and Heiko Ei{\ss}feldt and Marc Heuse},
  title     = {{AFL++} : Combining Incremental Steps of Fuzzing Research},
  booktitle = {14th USENIX Workshop on Offensive Technologies (WOOT 20)},
  year      = {2020},
  url       = {https://www.usenix.org/conference/woot20/presentation/fioraldi},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{AFLGo,
  author    = {B\"{o}hme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
  title     = {Directed Greybox Fuzzing},
  year      = {2017},
  isbn      = {9781450349468},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3133956.3134020},
  doi       = {10.1145/3133956.3134020},
  abstract  = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2329–2344},
  numpages  = {16},
  keywords  = {reachability, directed testing, coverage-based greybox fuzzing, verifying true positives, patch testing, crash reproduction},
  location  = {Dallas, Texas, USA},
  series    = {CCS '17}
}
@inproceedings{Munch,
  author    = {Ognawala, Saahil and Hutzelmann, Thomas and Psallida, Eirini and Pretschner, Alexander},
  title     = {Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach},
  year      = {2018},
  isbn      = {9781450351911},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3167132.3167289},
  doi       = {10.1145/3167132.3167289},
  abstract  = {Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open-source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage.},
  booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
  pages     = {1475–1482},
  numpages  = {8},
  keywords  = {symbolic execution, function coverage, compositional analysis, software testing, fuzzing},
  location  = {Pau, France},
  series    = {SAC '18}
}

@article{Magma,
  author     = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
  title      = {Magma: A Ground-Truth Fuzzing Benchmark},
  year       = {2021},
  issue_date = {December 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {4},
  number     = {3},
  url        = {https://doi.org/10.1145/3428334},
  doi        = {10.1145/3428334},
  abstract   = {High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count---perhaps the most commonly-used performance metric---is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.},
  journal    = {Proc. ACM Meas. Anal. Comput. Syst.},
  month      = {jun},
  articleno  = {49},
  numpages   = {29},
  keywords   = {benchmark, fuzzing, performance evaluation, software security}
}

@inproceedings{TFuzz,
  author    = {Peng, Hui and Shoshitaishvili, Yan and Payer, Mathias},
  booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  title     = {T-Fuzz: Fuzzing by Program Transformation},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {697-710},
  doi       = {10.1109/SP.2018.00056}
}

@inproceedings{LearnFuzz,
  author    = {Godefroid, Patrice and Peleg, Hila and Singh, Rishabh},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Learn\&Fuzz: Machine learning for input fuzzing},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {50-59},
  doi       = {10.1109/ASE.2017.8115618}
}

@article{FuzzingASurvey,
  title   = {Fuzzing: a survey},
  author  = {Jun Li and Bodong Zhao and Chao Zhang},
  journal = {Cybersecurity},
  year    = {2018},
  volume  = {1},
  pages   = {1-13},
  url     = {https://api.semanticscholar.org/CorpusID:46928493}
}

@article{FuzzingASurveyforRoadmap,
  author     = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
  title      = {Fuzzing: A Survey for Roadmap},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {11s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3512345},
  doi        = {10.1145/3512345},
  abstract   = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
  journal    = {ACM Comput. Surv.},
  month      = {sep},
  articleno  = {230},
  numpages   = {36},
  keywords   = {fuzzing theory, security, input space, Fuzz testing, automation}
}

@article{ArtScienceEngineeringFuzzing,
  author  = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  journal = {IEEE Transactions on Software Engineering},
  title   = {The Art, Science, and Engineering of Fuzzing: A Survey},
  year    = {2021},
  volume  = {47},
  number  = {11},
  pages   = {2312-2331},
  doi     = {10.1109/TSE.2019.2946563}
}

@article{Demystifying,
  author     = {Mallissery, Sanoop and Wu, Yu-Sung},
  title      = {Demystify the Fuzzing Methods: A Comprehensive Survey},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3623375},
  doi        = {10.1145/3623375},
  abstract   = {Massive software applications possess complex data structures or parse complex data structures; in such cases, vulnerabilities in the software become inevitable. The vulnerabilities are the source of cyber-security threats, and discovering this before the software deployment is challenging. Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. Each technique is wrapped beneath the black-, white-, and grey-box fuzzers to uncover diverse vulnerabilities. It consists of methods such as identifying structural information about the test cases to detect security vulnerabilities, symbolic and concrete program states to explore the unexplored locations, and full semantics of code coverage to create new test cases. We methodically examine each kind of fuzzers and contemporary fuzzers with a profound observation that addresses various research questions and systematically reviews and analyze the gaps and their solutions. Our survey comprised the recent related works on fuzzing techniques to demystify the fuzzing methods concerning the application domains and the target that, in turn, achieves higher code coverage and sound vulnerability detection.},
  journal    = {ACM Comput. Surv.},
  month      = {oct},
  articleno  = {71},
  numpages   = {38},
  keywords   = {vulnerability discovery, Automated testing, fuzzing, code inspection}
}

@article{FuzzingVulnerabilityDiscoveryTechniques,
  title    = {Fuzzing vulnerability discovery techniques: Survey, challenges and future directions},
  journal  = {Computers \& Security},
  volume   = {120},
  pages    = {102813},
  year     = {2022},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2022.102813},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404822002073},
  author   = {Craig Beaman and Michael Redbourne and J. Darren Mummery and Saqib Hakak},
  keywords = {Vulnerability, Fuzzing, Software Security, Fuzzers, Software Vulnerability, Vulnerability accessment, Static code analysis, Security},
  abstract = {Fuzzing is a powerful tool for vulnerability discovery in software, with much progress being made in the field in recent years. There is limited literature available on the fuzzing vulnerability discovery approaches. Hence, in this paper, an attempt has been made to explore the recent advances in the area of fuzzing vulnerability discovery and to propose a refinement to the classification of fuzzers. Furthermore, we have identified key research challenges and potential future areas of research that might provide new insight to researchers.}
}