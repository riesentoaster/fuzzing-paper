@article{UNIX,
  author     = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title      = {An Empirical Study of the Reliability of UNIX Utilities},
  year       = {1990},
  issue_date = {Dec. 1990},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/96267.96279},
  doi        = {10.1145/96267.96279},
  abstract   = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  journal    = {Commun. ACM},
  month      = {dec},
  pages      = {32–44},
  numpages   = {13}
}

@inproceedings{DART,
  author    = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  title     = {DART: Directed Automated Random Testing},
  year      = {2005},
  isbn      = {1595930566},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1065010.1065036},
  doi       = {10.1145/1065010.1065036},
  abstract  = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {213–223},
  numpages  = {11},
  keywords  = {software testing, interfaces, random testing, automated test generation, program verification},
  location  = {Chicago, IL, USA},
  series    = {PLDI '05}
}

@inproceedings{SAGE,
  title     = {Automated Whitebox Fuzz Testing},
  author    = {Patrice Godefroid and Michael Y. Levin and David A. Molnar},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2008},
  url       = {https://api.semanticscholar.org/CorpusID:1296783}
}

@inproceedings{GrammarBasedWhiteboxFuzzing,
  author    = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
  title     = {Grammar-Based Whitebox Fuzzing},
  year      = {2008},
  isbn      = {9781595938602},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1375581.1375607},
  doi       = {10.1145/1375581.1375607},
  abstract  = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages.In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {206–215},
  numpages  = {10},
  keywords  = {grammars, automatic test generation, software testing, program verification},
  location  = {Tucson, AZ, USA},
  series    = {PLDI '08}
}

@inproceedings{KLEE,
  author    = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  title     = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
  year      = {2008},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {209–224},
  numpages  = {16},
  location  = {San Diego, California},
  series    = {OSDI'08}
}

@article{Science,
  title    = {A systematic review of fuzzing techniques},
  journal  = {Computers \& Security},
  volume   = {75},
  pages    = {118-137},
  year     = {2018},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2018.02.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404818300658},
  author   = {Chen Chen and Baojiang Cui and Jinxin Ma and Runpu Wu and Jianchao Guo and Wenqian Liu},
  keywords = {Software bug, Vulnerability, Fuzzing, Dynamic symbolic execution, Coverage guide, Grammar representation, Scheduling algorithms, Taint analysis, Static analysis},
  abstract = {Fuzzing is an effective and widely used technique for finding security bugs and vulnerabilities in software. It inputs irregular test data into a target program to try to trigger a vulnerable condition in the program execution. Since the first random fuzzing system was constructed, fuzzing efficiency has been greatly improved by combination with several useful techniques, including dynamic symbolic execution, coverage guide, grammar representation, scheduling algorithms, dynamic taint analysis, static analysis and machine learning. In this paper, we will systematically review these techniques and their corresponding representative fuzzing systems. By introducing the principles, advantages and disadvantages of these techniques, we hope to provide researchers with a systematic and deeper understanding of fuzzing techniques and provide some references for this field.}
}

@inproceedings{Driller,
  title     = {Driller: Augmenting Fuzzing Through Selective Symbolic Execution},
  author    = {Nick Stephens and John Grosen and Christopher Salls and Andrew Dutcher and Ruoyu Wang and Jacopo Corbetta and Yan Shoshitaishvili and Christopher Kr{\"u}gel and Giovanni Vigna},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2016},
  url       = {https://api.semanticscholar.org/CorpusID:2388545}
}

@inproceedings{AFLPlusPlus,
  author    = {Andrea Fioraldi and Dominik Maier and Heiko Ei{\ss}feldt and Marc Heuse},
  title     = {{AFL++} : Combining Incremental Steps of Fuzzing Research},
  booktitle = {14th USENIX Workshop on Offensive Technologies (WOOT 20)},
  year      = {2020},
  url       = {https://www.usenix.org/conference/woot20/presentation/fioraldi},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{AFLGo,
  author    = {B\"{o}hme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
  title     = {Directed Greybox Fuzzing},
  year      = {2017},
  isbn      = {9781450349468},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3133956.3134020},
  doi       = {10.1145/3133956.3134020},
  abstract  = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2329–2344},
  numpages  = {16},
  keywords  = {reachability, directed testing, coverage-based greybox fuzzing, verifying true positives, patch testing, crash reproduction},
  location  = {Dallas, Texas, USA},
  series    = {CCS '17}
}
@inproceedings{Munch,
  author    = {Ognawala, Saahil and Hutzelmann, Thomas and Psallida, Eirini and Pretschner, Alexander},
  title     = {Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach},
  year      = {2018},
  isbn      = {9781450351911},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3167132.3167289},
  doi       = {10.1145/3167132.3167289},
  abstract  = {Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open-source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage.},
  booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
  pages     = {1475–1482},
  numpages  = {8},
  keywords  = {symbolic execution, function coverage, compositional analysis, software testing, fuzzing},
  location  = {Pau, France},
  series    = {SAC '18}
}

@article{Magma,
  author     = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
  title      = {Magma: A Ground-Truth Fuzzing Benchmark},
  year       = {2021},
  issue_date = {December 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {4},
  number     = {3},
  url        = {https://doi.org/10.1145/3428334},
  doi        = {10.1145/3428334},
  abstract   = {High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count---perhaps the most commonly-used performance metric---is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.},
  journal    = {Proc. ACM Meas. Anal. Comput. Syst.},
  month      = {jun},
  articleno  = {49},
  numpages   = {29},
  keywords   = {benchmark, fuzzing, performance evaluation, software security}
}

@inproceedings{TFuzz,
  author    = {Peng, Hui and Shoshitaishvili, Yan and Payer, Mathias},
  booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  title     = {T-Fuzz: Fuzzing by Program Transformation},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {697-710},
  doi       = {10.1109/SP.2018.00056}
}

@inproceedings{LearnFuzz,
  author    = {Godefroid, Patrice and Peleg, Hila and Singh, Rishabh},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Learn\&Fuzz: Machine learning for input fuzzing},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {50-59},
  doi       = {10.1109/ASE.2017.8115618}
}

@article{FuzzingASurvey,
  title   = {Fuzzing: a survey},
  author  = {Jun Li and Bodong Zhao and Chao Zhang},
  journal = {Cybersecurity},
  year    = {2018},
  volume  = {1},
  pages   = {1-13},
  url     = {https://api.semanticscholar.org/CorpusID:46928493}
}

@article{FuzzingASurveyforRoadmap,
  author     = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
  title      = {Fuzzing: A Survey for Roadmap},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {11s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3512345},
  doi        = {10.1145/3512345},
  abstract   = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
  journal    = {ACM Comput. Surv.},
  month      = {sep},
  articleno  = {230},
  numpages   = {36},
  keywords   = {fuzzing theory, security, input space, Fuzz testing, automation}
}

@article{ArtScienceEngineeringFuzzing,
  author  = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  journal = {IEEE Transactions on Software Engineering},
  title   = {The Art, Science, and Engineering of Fuzzing: A Survey},
  year    = {2021},
  volume  = {47},
  number  = {11},
  pages   = {2312-2331},
  doi     = {10.1109/TSE.2019.2946563}
}

@article{Demystifying,
  author     = {Mallissery, Sanoop and Wu, Yu-Sung},
  title      = {Demystify the Fuzzing Methods: A Comprehensive Survey},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3623375},
  doi        = {10.1145/3623375},
  abstract   = {Massive software applications possess complex data structures or parse complex data structures; in such cases, vulnerabilities in the software become inevitable. The vulnerabilities are the source of cyber-security threats, and discovering this before the software deployment is challenging. Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. Each technique is wrapped beneath the black-, white-, and grey-box fuzzers to uncover diverse vulnerabilities. It consists of methods such as identifying structural information about the test cases to detect security vulnerabilities, symbolic and concrete program states to explore the unexplored locations, and full semantics of code coverage to create new test cases. We methodically examine each kind of fuzzers and contemporary fuzzers with a profound observation that addresses various research questions and systematically reviews and analyze the gaps and their solutions. Our survey comprised the recent related works on fuzzing techniques to demystify the fuzzing methods concerning the application domains and the target that, in turn, achieves higher code coverage and sound vulnerability detection.},
  journal    = {ACM Comput. Surv.},
  month      = {oct},
  articleno  = {71},
  numpages   = {38},
  keywords   = {vulnerability discovery, Automated testing, fuzzing, code inspection}
}

@article{FuzzingVulnerabilityDiscoveryTechniques,
  title    = {Fuzzing vulnerability discovery techniques: Survey, challenges and future directions},
  journal  = {Computers \& Security},
  volume   = {120},
  pages    = {102813},
  year     = {2022},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2022.102813},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404822002073},
  author   = {Craig Beaman and Michael Redbourne and J. Darren Mummery and Saqib Hakak},
  keywords = {Vulnerability, Fuzzing, Software Security, Fuzzers, Software Vulnerability, Vulnerability accessment, Static code analysis, Security},
  abstract = {Fuzzing is a powerful tool for vulnerability discovery in software, with much progress being made in the field in recent years. There is limited literature available on the fuzzing vulnerability discovery approaches. Hence, in this paper, an attempt has been made to explore the recent advances in the area of fuzzing vulnerability discovery and to propose a refinement to the classification of fuzzers. Furthermore, we have identified key research challenges and potential future areas of research that might provide new insight to researchers.}
}

@inproceedings{TaintScope,
  author    = {Wang, Tielei and Wei, Tao and Gu, Guofei and Zou, Wei},
  booktitle = {2010 IEEE Symposium on Security and Privacy},
  title     = {TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {497-512},
  doi       = {10.1109/SP.2010.37}
}

@article{ReviewThreeDecades,
  author     = {Cadar, Cristian and Sen, Koushik},
  title      = {Symbolic Execution for Software Testing: Three Decades Later},
  year       = {2013},
  issue_date = {February 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/2408776.2408795},
  doi        = {10.1145/2408776.2408795},
  abstract   = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
  journal    = {Commun. ACM},
  month      = {feb},
  pages      = {82–90},
  numpages   = {9}
}

@article{CUTE,
  author     = {Sen, Koushik and Marinov, Darko and Agha, Gul},
  title      = {CUTE: A Concolic Unit Testing Engine for C},
  year       = {2005},
  issue_date = {September 2005},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {30},
  number     = {5},
  issn       = {0163-5948},
  url        = {https://doi.org/10.1145/1095430.1081750},
  doi        = {10.1145/1095430.1081750},
  abstract   = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  month      = {sep},
  pages      = {263–272},
  numpages   = {10},
  keywords   = {concolic testing, unit testing, random testing, testing C programs, data structure testing, explicit path model-checking}
}

@inproceedings{BuzzFuzz,
  author    = {Ganesh, Vijay and Leek, Tim and Rinard, Martin},
  booktitle = {2009 IEEE 31st International Conference on Software Engineering},
  title     = {Taint-based directed whitebox fuzzing},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {474-484},
  doi       = {10.1109/ICSE.2009.5070546}
}

@article{SAGEImpact,
  author     = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
  title      = {SAGE: Whitebox Fuzzing for Security Testing: SAGE Has Had a Remarkable Impact at Microsoft.},
  year       = {2012},
  issue_date = {January 2012},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {10},
  number     = {1},
  issn       = {1542-7730},
  url        = {https://doi.org/10.1145/2090147.2094081},
  doi        = {10.1145/2090147.2094081},
  abstract   = {Most ACM Queue readers might think of "program verification research" as mostly theoretical with little impact on the world at large. Think again. If you are reading these lines on a PC running some form of Windows (like 93-plus percent of PC users--that is, more than a billion people), then you have been affected by this line of work--without knowing it, which is precisely the way we want it to be.},
  journal    = {Queue},
  month      = {jan},
  pages      = {20–27},
  numpages   = {8}
}

@article{DowserArticle,
  title   = {Dowser: A Guided Fuzzer for Finding Buffer Overflow Vulnerabilities},
  author  = {Istv{\'a}n Haller and Asia Slowinska and Matthias Neugschwandtner and Herbert Bos},
  journal = {login Usenix Mag.},
  year    = {2013},
  volume  = {38},
  url     = {https://api.semanticscholar.org/CorpusID:56594822}
}

@inproceedings{Dowser,
  author    = {Haller, Istvan and Slowinska, Asia and Neugschwandtner, Matthias and Bos, Herbert},
  title     = {Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations},
  year      = {2013},
  isbn      = {9781931971034},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {Dowser is a 'guided' fuzzer that combines taint tracking, program analysis and symbolic execution to find buffer overflow and underflow vulnerabilities buried deep in a program's logic. The key idea is that analysis of a program lets us pinpoint the right areas in the program code to probe and the appropriate inputs to do so.Intuitively, for typical buffer overflows, we need consider only the code that accesses an array in a loop, rather than all possible instructions in the program. After finding all such candidate sets of instructions, we rank them according to an estimation of how likely they are to contain interesting vulnerabilities. We then subject the most promising sets to further testing. Specifically, we first use taint analysis to determine which input bytes influence the array index and then execute the program symbolically, making only this set of inputs symbolic. By constantly steering the symbolic execution along branch outcomes most likely to lead to overflows, we were able to detect deep bugs in real programs (like the nginx webserver, the inspircd IRC server, and the ffmpeg videoplayer). Two of the bugs we found were previously undocumented buffer overflows in ffmpeg and the poppler PDF rendering library.},
  booktitle = {Proceedings of the 22nd USENIX Conference on Security},
  pages     = {49–64},
  numpages  = {16},
  location  = {Washington, D.C.},
  series    = {SEC'13}
}

@inproceedings{BORG,
  author    = {Neugschwandtner, Matthias and Milani Comparetti, Paolo and Haller, Istvan and Bos, Herbert},
  title     = {The BORG: Nanoprobing Binaries for Buffer Overreads},
  year      = {2015},
  isbn      = {9781450331913},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2699026.2699098},
  doi       = {10.1145/2699026.2699098},
  abstract  = {Automated program testing tools typically try to explore, and cover, as much of a tested program as possible, while attempting to trigger and detect bugs. An alternative and complementary approach can be to first select a specific part of a program that may be subject to a specific class of bug, and then narrowly focus exploration towards program paths that could trigger such a bug.In this work, we introduce the BORG (Buffer Over-Read Guard), a testing tool that uses static and dynamic program analysis, taint propagation and symbolic execution to detect buffer overread bugs in real-world programs. BORG works by first selecting buffer accesses that could lead to an overread and then guiding symbolic execution towards those accesses along program paths that could actually lead to an overread. BORG operates on binaries and does not require source code. To demonstrate BORG's effectiveness, we use it to detect overreads in six complex server applications and libraries, including lighttpd, FFmpeg and ClamAV.},
  booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
  pages     = {87–97},
  numpages  = {11},
  keywords  = {out-of-bounds access, symbolic execution guidance, dynamic symbolic execution, buffer overread, targeted testing},
  location  = {San Antonio, Texas, USA},
  series    = {CODASPY '15}
}


@inproceedings{MoWF,
  author    = {Pham, Van-Thuan and B\"{o}hme, Marcel and Roychoudhury, Abhik},
  title     = {Model-Based Whitebox Fuzzing for Program Binaries},
  year      = {2016},
  isbn      = {9781450338455},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2970276.2970316},
  doi       = {10.1145/2970276.2970316},
  abstract  = {Many real-world programs take highly structured and very complex inputs. The automated testing of such programs is non-trivial. If the test input does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths leading to trivial parser errors. Naturally, the time is better spent exploring the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and the data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Model-based Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input space to rule out most invalid inputs during path exploration in symbolic execution. We evaluate on 13 vulnerabilities in 8 large program binaries with 6 separate file formats and found that MoWF exposes all vulnerabilities while both, traditional whitebox fuzzing and model-based blackbox fuzzing, expose only less than half, respectively. Our experiments also demonstrate that MoWF exposes 70\% vulnerabilities without any seed inputs.},
  booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
  pages     = {543–553},
  numpages  = {11},
  keywords  = {Symbolic Execution, Program Binaries},
  location  = {Singapore, Singapore},
  series    = {ASE '16}
}

@article{S2E,
  author     = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
  title      = {S2E: A Platform for in-Vivo Multi-Path Analysis of Software Systems},
  year       = {2011},
  issue_date = {March 2011},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {3},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1961296.1950396},
  doi        = {10.1145/1961296.1950396},
  abstract   = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
  journal    = {SIGPLAN Not.},
  month      = {mar},
  pages      = {265–278},
  numpages   = {14},
  keywords   = {dbt, virtualization, consistency models, testing, symbolic execution, binary, analysis, performance, in-vivo, framework}
}

@inproceedings{Mayhem,
  author    = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
  booktitle = {2012 IEEE Symposium on Security and Privacy},
  title     = {Unleashing Mayhem on Binary Code},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {380-394},
  doi       = {10.1109/SP.2012.31}
}

@inproceedings{VUzzer,
  title     = {VUzzer: Application-aware Evolutionary Fuzzing},
  author    = {Sanjay Rawat and Vivek Jain and Ashish Kumar and Lucian Cojocar and Cristiano Giuffrida and Herbert Bos},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2017},
  url       = {https://api.semanticscholar.org/CorpusID:2354736}
}

@inproceedings{SYMFUZZ,
  author    = {Cha, Sang Kil and Woo, Maverick and Brumley, David},
  booktitle = {2015 IEEE Symposium on Security and Privacy},
  title     = {Program-Adaptive Mutational Fuzzing},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {725-741},
  doi       = {10.1109/SP.2015.50}
}

@inproceedings{EvaluatingFuzzTesting,
  author    = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
  title     = {Evaluating Fuzz Testing},
  year      = {2018},
  isbn      = {9781450356930},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3243734.3243804},
  doi       = {10.1145/3243734.3243804},
  abstract  = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
  booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2123–2138},
  numpages  = {16},
  keywords  = {fuzzing, evaluation, security},
  location  = {Toronto, Canada},
  series    = {CCS '18}
}

@article{HackArtScience,
  author     = {Godefroid, Patrice},
  title      = {Fuzzing: Hack, Art, and Science},
  year       = {2020},
  issue_date = {February 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {63},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3363824},
  doi        = {10.1145/3363824},
  abstract   = {Reviewing software testing techniques for finding security vulnerabilities.},
  journal    = {Commun. ACM},
  month      = {jan},
  pages      = {70–76},
  numpages   = {7}
}

@article{FuzzingTheStateOfTheArt,
  title   = {Fuzzing: The State of the Art},
  journal = {DSTO Defence Science and Technology Organisation},
  author  = {Richard McNally and Kenneth Kwok-Hei Yiu and Duncan A. Grove and Damien Gerhardy},
  year    = {2012},
  url     = {https://api.semanticscholar.org/CorpusID:15447929}
}

@article{SystematicReview2023,
  title     = {A systematic review of fuzzing},
  author    = {Zhao, Xiaoqi and Qu, Haipeng and Xu, Jianliang and Li, Xiaohui and Lv, Wenjie and Wang, Gai-Ge},
  journal   = {Soft Computing},
  pages     = {1--30},
  year      = {2023},
  publisher = {Springer}
}

@article{IoT,
  author  = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
  journal = {IEEE Internet of Things Journal},
  title   = {Fuzzing the Internet of Things: A Review on the Techniques and Challenges for Efficient Vulnerability Discovery in Embedded Systems},
  year    = {2021},
  volume  = {8},
  number  = {13},
  pages   = {10390-10411},
  doi     = {10.1109/JIOT.2021.3056179}
}

@article{ChallengesAndReflections,
  author  = {Boehme, Marcel and Cadar, Cristian and ROYCHOUDHURY, Abhik},
  journal = {IEEE Software},
  title   = {Fuzzing: Challenges and Reflections},
  year    = {2021},
  volume  = {38},
  number  = {3},
  pages   = {79-86},
  doi     = {10.1109/MS.2020.3016773}
}

@article{Network,
  author   = {Munea, Tewodros Legesse and Lim, Hyunwoo and Shon, Taeshik},
  title    = {Network protocol fuzz testing for information systems and applications: a survey and taxonomy},
  journal  = {Multimedia Tools and Applications},
  year     = {2016},
  month    = {Nov},
  day      = {01},
  volume   = {75},
  number   = {22},
  pages    = {14745-14757},
  abstract = {Fuzzing or fuzz testing has been introduced as a software testing technique to reduce vulnerabilities in software systems or given targets. To achieve a maximum benefit-to-cost ratio and without complication, we use fuzz testing [11]. In addition, during the development and debugging of a system, we may fail to notice the kinds of shortcoming that fuzz testing can expose. Fuzz testing types are different depending on the target they fuzz. Application, file format, and protocol fuzzing are the most common fuzzing types. A protocol fuzzer sends counterfeit packets to a target system while changing the normal packet en-route and sometimes replaying them. In addition, a protocol fuzzer sometimes acts as proxy server for clients. This survey study examines network protocol fuzz testing. We identified several studies on network protocol fuzzing. Most focus on application layers of the Open Systems Interconnection model. We primarily review the approaches of five studies and the targets and protocol layers they fuzz. We then develop criteria to compare these approaches in detail.},
  issn     = {1573-7721},
  doi      = {10.1007/s11042-015-2763-6},
  url      = {https://doi.org/10.1007/s11042-015-2763-6}
}

@article{FuzzingStateOfTheArt2018,
  author  = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
  journal = {IEEE Transactions on Reliability},
  title   = {Fuzzing: State of the Art},
  year    = {2018},
  volume  = {67},
  number  = {3},
  pages   = {1199-1218},
  doi     = {10.1109/TR.2018.2834476}
}

@inproceedings{Firmware,
  author    = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
  title     = {Firmware Fuzzing: The State of the Art},
  year      = {2021},
  isbn      = {9781450388191},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3457913.3457934},
  doi       = {10.1145/3457913.3457934},
  abstract  = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
  booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
  pages     = {110–115},
  numpages  = {6},
  keywords  = {literature review, firmware, Internet of Things, fuzzing},
  location  = {Singapore, Singapore},
  series    = {Internetware '20}
}

@article{Embedded,
  author     = {Yun, Joobeom and Rustamov, Fayozbek and Kim, Juhwan and Shin, Youngjoo},
  title      = {Fuzzing of Embedded Systems: A Survey},
  year       = {2022},
  issue_date = {July 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {55},
  number     = {7},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3538644},
  doi        = {10.1145/3538644},
  abstract   = {Security attacks abuse software vulnerabilities of IoT devices; hence, detecting and eliminating these vulnerabilities immediately are crucial. Fuzzing is an efficient method to identify vulnerabilities automatically, and many publications have been released to date. However, fuzzing for embedded systems has not been studied extensively owing to various obstacles, such as multi-architecture support, crash detection difficulties, and limited resources. Thus, the article introduces fuzzing techniques for embedded systems and the fuzzing differences for desktop and embedded systems. Further, we collect state-of-the-art technologies, discuss their advantages and disadvantages, and classify embedded system fuzzing tools. Finally, future directions for fuzzing research of embedded systems are predicted and discussed.},
  journal    = {ACM Comput. Surv.},
  month      = {dec},
  articleno  = {137},
  numpages   = {33},
  keywords   = {embedded systems, fuzzing, software testing, Firmware fuzzing, symbolic execution, concolic execution, IoT devices, firmware analysis}
}

@inproceedings{JavaScript,
  author    = {Tian, Ye and Qin, Xiaojun and Gan, Shuitao},
  title     = {Research on Fuzzing Technology for JavaScript Engines},
  year      = {2021},
  isbn      = {9781450389853},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3487075.3487107},
  doi       = {10.1145/3487075.3487107},
  abstract  = {JavaScript engine is the core component of web browsers, whose security issues are one of the critical aspects of the overall Web Eco-Security. Fuzzing technology, as an efficient software testing approach, has been widely applied to detecting vulnerabilities in different JavaScript engines, which is a security research hotspot at present. Based on systematical dissection of existing fuzzing methods, this paper reviews the development and technical ideas of JavaScript Engine Fuzzing combined with taxonomy, proposes a general framework of JavaScript Engine Fuzzing and analyzes the key techniques involved. Finally, we discuss the core issues that restrict efficiency in current research and present an outlook on the future trends of JavaScript Engine Fuzzing.},
  booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
  articleno = {32},
  numpages  = {7},
  keywords  = {Fuzzing, Browser security, JavaScript engine, Vulnerability detection},
  location  = {Sanya, China},
  series    = {CSAE '21}
}
@article{SearchStrategies,
  title    = {A Systematic Review of Search Strategies in Dynamic Symbolic Execution},
  journal  = {Computer Standards \& Interfaces},
  volume   = {72},
  pages    = {103444},
  year     = {2020},
  issn     = {0920-5489},
  doi      = {https://doi.org/10.1016/j.csi.2020.103444},
  url      = {https://www.sciencedirect.com/science/article/pii/S0920548919300066},
  author   = {Arash Sabbaghi and Mohammad Reza Keyvanpour},
  keywords = {Software Quality, Software Testing, Dynamic Symbolic Execution, Concolic Testing, Execution Generated Testing, Search Strategy},
  abstract = {One of the major concerns of dynamic symbolic execution (DSE) based automated test case generation is its huge search space which restricts its usage for industrial-size program testing. In fact, DSE performs test case generation by exploring paths of the program, and the number of program paths is exponential in the number of branch conditions encountered during execution. Thus, by increasing the number of branches, the search space will be extremely large and without applying an effective and efficient technique to explore the search space, DSE would fail to achieve the predesignated goals in the given budget. To this end, different search strategies have been proposed to prioritize program paths and to select the most promising ones with respect to the testing goal. In this paper, we conduct a comprehensive systematic review of search strategies in DSE. We collect different techniques and methods concerning the topic, classify and summarize them, highlight their advantages and drawbacks, and provide a complete comparison of the methods in each category. The classification is carried out according to the type of search and also the information source exploited by the strategies to direct DSE. We also analyze the evaluation methodologies of experiments reported on this subject, give a general overview of them, perform a set of experiments and provide a set of guidelines for conducting future experiments in this area of research.}
}

@inproceedings{PreliminaryAssessment,
  author    = {Cadar, Cristian and Godefroid, Patrice and Khurshid, Sarfraz and P\u{a}s\u{a}reanu, Corina S. and Sen, Koushik and Tillmann, Nikolai and Visser, Willem},
  title     = {Symbolic Execution for Software Testing in Practice: Preliminary Assessment},
  year      = {2011},
  isbn      = {9781450304450},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1985793.1985995},
  doi       = {10.1145/1985793.1985995},
  abstract  = {We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.},
  booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
  pages     = {1066–1071},
  numpages  = {6},
  keywords  = {generalized symbolic execution, dynamic test generation},
  location  = {Waikiki, Honolulu, HI, USA},
  series    = {ICSE '11}
}

@article{EXE,
  author     = {Cadar, Cristian and Ganesh, Vijay and Pawlowski, Peter M. and Dill, David L. and Engler, Dawson R.},
  title      = {EXE: Automatically Generating Inputs of Death},
  year       = {2008},
  issue_date = {December 2008},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {12},
  number     = {2},
  issn       = {1094-9224},
  url        = {https://doi.org/10.1145/1455518.1455522},
  doi        = {10.1145/1455518.1455522},
  abstract   = {This article presents EXE, an effective bug-finding tool that automatically generates inputs that crash real code. Instead of running code on manually or randomly constructed input, EXE runs it on symbolic input initially allowed to be anything. As checked code runs, EXE tracks the constraints on each symbolic (i.e., input-derived) memory location. If a statement uses a symbolic value, EXE does not run it, but instead adds it as an input-constraint; all other statements run as usual. If code conditionally checks a symbolic expression, EXE forks execution, constraining the expression to be true on the true branch and false on the other. Because EXE reasons about all possible values on a path, it has much more power than a traditional runtime tool: (1) it can force execution down any feasible program path and (2) at dangerous operations (e.g., a pointer dereference), it detects if the current path constraints allow any value that causes a bug. When a path terminates or hits a bug, EXE automatically generates a test case by solving the current path constraints to find concrete values using its own co-designed constraint solver, STP. Because EXE’s constraints have no approximations, feeding this concrete input to an uninstrumented version of the checked code will cause it to follow the same path and hit the same bug (assuming deterministic code).EXE works well on real code, finding bugs along with inputs that trigger them in: the BSD and Linux packet filter implementations, the dhcpd DHCP server, the pcre regular expression library, and three Linux file systems.},
  journal    = {ACM Trans. Inf. Syst. Secur.},
  month      = {dec},
  articleno  = {10},
  numpages   = {38},
  keywords   = {test case generation, constraint solving, symbolic execution, attack generation, dynamic analysis, bug finding}
}

@inproceedings{RWset,
  author    = {Boonstoppel, Peter and Cadar, Cristian and Engler, Dawson},
  title     = {RWset: Attacking Path Explosion in Constraint-Based Test Generation},
  year      = {2008},
  isbn      = {3540787992},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg},
  abstract  = {Recent work has used variations of symbolic execution to automatically generate high-coverage test inputs [3, 4, 7, 8, 14]. Such tools have demonstrated their ability to find very subtle errors. However, one challenge they all face is how to effectively handle the exponential number of paths in checked code. This paper presents a new technique for reducing the number of traversed code paths by discarding those that must have side-effects identical to some previously explored path. Our results on a mix of open source applications and device drivers show that this (sound) optimization reduces the numbers of paths traversed by several orders of magnitude, often achieving program coverage far out of reach for a standard constraint-based execution system.},
  booktitle = {Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems},
  pages     = {351–366},
  numpages  = {16},
  location  = {Budapest, Hungary},
  series    = {TACAS'08/ETAPS'08}
}

@inproceedings{CREST,
  author    = {Burnim, Jacob and Sen, Koushik},
  booktitle = {2008 23rd IEEE/ACM International Conference on Automated Software Engineering},
  title     = {Heuristics for Scalable Dynamic Test Generation},
  year      = {2008},
  volume    = {},
  number    = {},
  pages     = {443-446},
  doi       = {10.1109/ASE.2008.69}
}

@inproceedings{BuildItBreakItFixIt,
  author    = {Ruef, Andrew and Hicks, Michael and Parker, James and Levin, Dave and Mazurek, Michelle L. and Mardziel, Piotr},
  title     = {Build It, Break It, Fix It: Contesting Secure Development},
  year      = {2016},
  isbn      = {9781450341394},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2976749.2978382},
  doi       = {10.1145/2976749.2978382},
  abstract  = {Typical security contests focus on breaking or mitigating the impact of buggy systems. We present the Build-it, Break-it, Fix-it (BIBIFI) contest, which aims to assess the ability to securely build software, not just break it. In BIBIFI, teams build specified software with the goal of maximizing correctness, performance, and security. The latter is tested when teams attempt to break other teams' submissions. Winners are chosen from among the best builders and the best breakers. BIBIFI was designed to be open-ended-teams can use any language, tool, process, etc. that they like. As such, contest outcomes shed light on factors that correlate with successfully building secure software and breaking insecure software. During 2015, we ran three contests involving a total of 116 teams and two different programming problems. Quantitative analysis from these contests found that the most efficient build-it submissions used C/C++, but submissions coded in other statically-typed languages were less likely to have a security flaw; build-it teams with diverse programming-language knowledge also produced more secure code. Shorter programs correlated with better scores. Break-it teams that were also successful build-it teams were significantly better at finding security bugs.},
  booktitle = {Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {690–703},
  numpages  = {14},
  keywords  = {software engineering, security},
  location  = {Vienna, Austria},
  series    = {CCS '16}
}

@inproceedings{FIXREVERTER,
  author    = {Zenong Zhang and Zach Patterson and Michael Hicks and Shiyi Wei},
  title     = {{FIXREVERTER}: A Realistic Bug Injection Methodology for Benchmarking Fuzz Testing},
  booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
  year      = {2022},
  isbn      = {978-1-939133-31-1},
  address   = {Boston, MA},
  pages     = {3699--3715},
  url       = {https://www.usenix.org/conference/usenixsecurity22/presentation/zhang-zenong},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{UnTracer,
  author    = {Nagy, Stefan and Hicks, Matthew},
  booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
  title     = {Full-Speed Fuzzing: Reducing Fuzzing Overhead through Coverage-Guided Tracing},
  year      = {2019},
  volume    = {},
  number    = {},
  pages     = {787-802},
  doi       = {10.1109/SP.2019.00069}
}

@inproceedings{ZAFL,
  author    = {Stefan Nagy and Anh Nguyen-Tuong and Jason D. Hiser and Jack W. Davidson and Matthew Hicks},
  title     = {Breaking Through Binaries: Compiler-quality Instrumentation for Better Binary-only Fuzzing},
  booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
  year      = {2021},
  isbn      = {978-1-939133-24-3},
  pages     = {1683--1700},
  url       = {https://www.usenix.org/conference/usenixsecurity21/presentation/nagy},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{HardwareFuzzingPipeline,
  author    = {Timothy Trippel and Kang G. Shin and Alex Chernyakhovsky and Garret Kelly and Dominic Rizzo and Matthew Hicks},
  title     = {Fuzzing Hardware Like Software},
  booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
  year      = {2022},
  isbn      = {978-1-939133-31-1},
  address   = {Boston, MA},
  pages     = {3237--3254},
  url       = {https://www.usenix.org/conference/usenixsecurity22/presentation/trippel},
  publisher = {USENIX Association},
  month     = aug
}

@article{CGPT,
  author     = {Lampropoulos, Leonidas and Hicks, Michael and Pierce, Benjamin C.},
  title      = {Coverage Guided, Property Based Testing},
  year       = {2019},
  issue_date = {October 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3360607},
  doi        = {10.1145/3360607},
  abstract   = {Property-based random testing, exemplified by frameworks such as Haskell's QuickCheck, works by testing an executable predicate (a property) on a stream of randomly generated inputs. Property testing works very well in many cases, but not always. Some properties are conditioned on the input satisfying demanding semantic invariants that are not consequences of its syntactic structure---e.g., that an input list must be sorted or have no duplicates. Most randomly generated inputs fail to satisfy properties with such sparse preconditions, and so are simply discarded. As a result, much of the target system may go untested. We address this issue with a novel technique called coverage guided, property based testing (CGPT). Our approach is inspired by the related area of coverage guided fuzzing, exemplified by tools like AFL. Rather than just generating a fresh random input at each iteration, CGPT can also produce new inputs by mutating previous ones using type-aware, generic mutator operators. The target program is instrumented to track which control flow branches are executed during a run and inputs whose runs expand control-flow coverage are retained for future mutations. This means that, when sparse conditions in the target are satisfied and new coverage is observed, the input that triggered them will be retained and used as a springboard to go further. We have implemented CGPT as an extension to the QuickChick property testing tool for Coq programs; we call our implementation FuzzChick. We evaluate FuzzChick on two Coq developments for abstract machines that aim to enforce flavors of noninterference, which has a (very) sparse precondition. We systematically inject bugs in the machines' checking rules and use FuzzChick to look for counterexamples to the claim that they satisfy a standard noninterference property. We find that vanilla QuickChick almost always fails to find any bugs after a long period of time, as does an earlier proposal for combining property testing and fuzzing. In contrast, FuzzChick often finds them within seconds to minutes. Moreover, FuzzChick is almost fully automatic; although highly tuned, hand-written generators can find the bugs faster, they require substantial amounts of insight and manual effort.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {181},
  numpages   = {29},
  keywords   = {coverage, fuzz testing, FuzzChick, random testing, QuickChick, property-based testing, AFL}
}

@inproceedings{KATCH,
  author    = {Marinescu, Paul Dan and Cadar, Cristian},
  title     = {KATCH: High-Coverage Testing of Software Patches},
  year      = {2013},
  isbn      = {9781450322379},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2491411.2491438},
  doi       = {10.1145/2491411.2491438},
  abstract  = {One of the distinguishing characteristics of software systems is that they evolve: new patches are committed to software repositories and new versions are released to users on a continuous basis. Unfortunately, many of these changes bring unexpected bugs that break the stability of the system or affect its security. In this paper, we address this problem using a technique for automatically testing code patches. Our technique combines symbolic execution with several novel heuristics based on static and dynamic program analysis which allow it to quickly reach the code of the patch. We have implemented our approach in a tool called KATCH, which we have applied to all the patches written in a combined period of approximately six years for nineteen mature programs from the popular GNU diffutils, GNU binutils and GNU findutils utility suites, which are shipped with virtually all UNIX-based distributions. Our results show that KATCH can automatically synthesise inputs that significantly increase the patch coverage achieved by the existing manual test suites, and find bugs at the moment they are introduced.},
  booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
  pages     = {235–245},
  numpages  = {11},
  keywords  = {Patch Testing, Symbolic Execution},
  location  = {Saint Petersburg, Russia},
  series    = {ESEC/FSE 2013}
}

@inproceedings{KLEEFP,
  author    = {Collingbourne, Peter and Cadar, Cristian and Kelly, Paul H.J.},
  title     = {Symbolic Crosschecking of Floating-Point and SIMD Code},
  year      = {2011},
  isbn      = {9781450306348},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1966445.1966475},
  doi       = {10.1145/1966445.1966475},
  abstract  = {We present an effective technique for crosschecking an IEEE 754 floating-point program and its SIMD-vectorized version, implemented in KLEE-FP, an extension to the KLEE symbolic execution tool that supports symbolic reasoning on the equivalence between floating-point values.The key insight behind our approach is that floatingpoint values are only reliably equal if they are essentially built by the same operations. As a result, our technique works by lowering the Intel Streaming SIMD Extension (SSE) instruction set to primitive integer and floating-point operations, and then using an algorithm based on symbolic expression matching augmented with canonicalization rules.Under symbolic execution, we have to verify equivalence along every feasible control-flow path. We reduce the branching factor of this process by aggressively merging conditionals, if-converting branches into select operations via an aggressive phi-node folding transformation.We applied KLEE-FP to OpenCV, a popular open source computer vision library. KLEE-FP was able to successfully crosscheck 51 SIMD/SSE implementations against their corresponding scalar versions, proving the bounded equivalence of 41 of them (i.e., on images up to a certain size), and finding inconsistencies in the other 10.},
  booktitle = {Proceedings of the Sixth Conference on Computer Systems},
  pages     = {315–328},
  numpages  = {14},
  keywords  = {sse, bounded verification, simd, klee-fp, symbolic crosschecking},
  location  = {Salzburg, Austria},
  series    = {EuroSys '11}
}

@inproceedings{Covrig,
  author    = {Marinescu, Paul and Hosek, Petr and Cadar, Cristian},
  title     = {Covrig: A Framework for the Analysis of Code, Test, and Coverage Evolution in Real Software},
  year      = {2014},
  isbn      = {9781450326452},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2610384.2610419},
  doi       = {10.1145/2610384.2610419},
  abstract  = {Software repositories provide rich information about the construction and evolution of software systems. While static data that can be mined directly from version control systems has been extensively studied, dynamic metrics concerning the execution of the software have received much less attention, due to the inherent difficulty of running and monitoring a large number of software versions. In this paper, we present Covrig, a flexible infrastructure that can be used to run each version of a system in isolation and collect static and dynamic software metrics, using a lightweight virtual machine environment that can be deployed on a cluster of local or cloud machines. We use Covrig to conduct an empirical study examining how code and tests co-evolve in six popular open-source systems. We report the main characteristics of software patches, analyse the evolution of program and patch coverage, assess the impact of nondeterminism on the execution of test suites, and investigate whether the coverage of code containing bugs and bug fixes is higher than average.},
  booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
  pages     = {93–104},
  numpages  = {12},
  keywords  = {bugs and fixes, coverage evolution, latent patch cover- age, nondeterministic coverage, Patch characteristics},
  location  = {San Jose, CA, USA},
  series    = {ISSTA 2014}
}

@inproceedings{AutomaticTestingSymbex,
  author    = {Kapus, Timotej and Cadar, Cristian},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Automatic testing of symbolic execution engines via program generation and differential testing},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {590-600},
  doi       = {10.1109/ASE.2017.8115669}
}

@inproceedings{JFS,
  author    = {Liew, Daniel and Cadar, Cristian and Donaldson, Alastair F. and Stinnett, J. Ryan},
  title     = {Just Fuzz It: Solving Floating-Point Constraints Using Coverage-Guided Fuzzing},
  year      = {2019},
  isbn      = {9781450355728},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3338906.3338921},
  doi       = {10.1145/3338906.3338921},
  abstract  = {We investigate the use of coverage-guided fuzzing as a means of proving satisfiability of SMT formulas over finite variable domains, with specific application to floating-point constraints. We show how an SMT formula can be encoded as a program containing a location that is reachable if and only if the program’s input corresponds to a satisfying assignment to the formula. A coverage-guided fuzzer can then be used to search for an input that reaches the location, yielding a satisfying assignment. We have implemented this idea in a tool, Just Fuzz-it Solver (JFS), and we present a large experimental evaluation showing that JFS is both competitive with and complementary to state-of-the-art SMT solvers with respect to solving floating-point constraints, and that the coverage-guided approach of JFS provides significant benefit over naive fuzzing in the floating-point domain. Applied in a portfolio manner, the JFS approach thus has the potential to complement traditional SMT solvers for program analysis tasks that involve reasoning about floating-point constraints.},
  booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages     = {521–532},
  numpages  = {12},
  keywords  = {feedback-directed fuzzing, Constraint solving},
  location  = {Tallinn, Estonia},
  series    = {ESEC/FSE 2019}
}

@inproceedings{ZESTI,
  author    = {Dan Marinescu, Paul and Cadar, Cristian},
  booktitle = {2012 34th International Conference on Software Engineering (ICSE)},
  title     = {make test-zesti: A symbolic execution solution for improving regression testing},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {716-726},
  doi       = {10.1109/ICSE.2012.6227146}
}

@inproceedings{ZEST,
  author    = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Papadakis, Mike and Le Traon, Yves},
  title     = {Semantic Fuzzing with Zest},
  year      = {2019},
  isbn      = {9781450362245},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3293882.3330576},
  doi       = {10.1145/3293882.3330576},
  abstract  = {Programs expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like random input generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric input generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks' semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug.},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {329–340},
  numpages  = {12},
  keywords  = {property-based testing, Structure-aware fuzzing, random testing},
  location  = {Beijing, China},
  series    = {ISSTA 2019}
}
@article{FuzzFactory,
  author     = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
  title      = {FuzzFactory: Domain-Specific Fuzzing with Waypoints},
  year       = {2019},
  issue_date = {October 2019},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {3},
  number     = {OOPSLA},
  url        = {https://doi.org/10.1145/3360600},
  doi        = {10.1145/3360600},
  abstract   = {Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern. In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.},
  journal    = {Proc. ACM Program. Lang.},
  month      = {oct},
  articleno  = {174},
  numpages   = {29},
  keywords   = {waypoints, fuzz testing, frameworks, domain-specific fuzzing}
}

@inproceedings{FairFuzz,
  author    = {Lemieux, Caroline and Sen, Koushik},
  title     = {FairFuzz: A Targeted Mutation Strategy for Increasing Greybox Fuzz Testing Coverage},
  year      = {2018},
  isbn      = {9781450359375},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3238147.3238176},
  doi       = {10.1145/3238147.3238176},
  abstract  = {In recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice. One particular fuzz testing tool, American Fuzzy Lop (AFL), has become popular thanks to its ease-of-use and bug-finding power. However, AFL remains limited in the bugs it can find since it simply does not cover large regions of code. If it does not cover parts of the code, it will not find bugs there. We propose a two-pronged approach to increase the coverage achieved by AFL. First, the approach automatically identifies branches exercised by few AFL-produced inputs (rare branches), which often guard code that is empirically hard to cover by naively mutating inputs. The second part of the approach is a novel mutation mask creation algorithm, which allows mutations to be biased towards producing inputs hitting a given rare branch. This mask is dynamically computed during fuzz testing and can be adapted to other testing targets. We implement this approach on top of AFL in a tool named FairFuzz. We conduct evaluation on real-world programs against state-of-the-art versions of AFL. We find that on these programs FairFuzz achieves high branch coverage at a faster rate that state-of-the-art versions of AFL. In addition, on programs with nested conditional structure, it achieves sustained increases in branch coverage after 24 hours (average 10.6\% increase). In qualitative analysis, we find that FairFuzz has an increased capacity to automatically discover keywords.},
  booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
  pages     = {475–485},
  numpages  = {11},
  keywords  = {coverage-guided greybox fuzzing, fuzz testing, rare branches},
  location  = {Montpellier, France},
  series    = {ASE '18}
}

@inproceedings{JQF,
  author    = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik},
  title     = {JQF: Coverage-Guided Property-Based Testing in Java},
  year      = {2019},
  isbn      = {9781450362245},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3293882.3339002},
  doi       = {10.1145/3293882.3339002},
  abstract  = {We present JQF, a platform for performing coverage-guided fuzz testing in Java. JQF is designed both for practitioners, who wish to find bugs in Java programs, as well as for researchers, who wish to implement new fuzzing algorithms. Practitioners write QuickCheck-style test methods that take inputs as formal parameters. JQF instruments the test program's bytecode and continuously executes tests using inputs that are generated in a coverage-guided fuzzing loop. JQF's input-generation mechanism is extensible. Researchers can implement custom fuzzing algorithms by extending JQF's Guidance interface. A Guidance instance responds to code coverage events generated during the execution of a test case, such as function calls and conditional jumps, and provides the next input. We describe several guidances that currently ship with JQF, such as: semantic fuzzing with Zest, binary fuzzing with AFL, and complexity fuzzing with PerfFuzz. JQF is a mature tool that is open-source and publicly available. At the time of writing, JQF has been successful in discovering 42 previously unknown bugs in widely used open-source software such as OpenJDK, Apache Commons, and the Google Closure Compiler.},
  booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {398–401},
  numpages  = {4},
  keywords  = {property-based testing, QuickCheck, Coverage-guided fuzzing},
  location  = {Beijing, China},
  series    = {ISSTA 2019}
}

@inproceedings{PerfFuzz,
  author    = {Lemieux, Caroline and Padhye, Rohan and Sen, Koushik and Song, Dawn},
  title     = {PerfFuzz: Automatically Generating Pathological Inputs},
  year      = {2018},
  isbn      = {9781450356992},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3213846.3213874},
  doi       = {10.1145/3213846.3213874},
  abstract  = {Performance problems in software can arise unexpectedly when programs are provided with inputs that exhibit worst-case behavior. A large body of work has focused on diagnosing such problems via statistical profiling techniques. But how does one find these inputs in the first place? We present PerfFuzz, a method to automatically generate inputs that exercise pathological behavior across program locations, without any domain knowledge. PerfFuzz generates inputs via feedback-directed mutational fuzzing. Unlike previous approaches that attempt to maximize only a scalar characteristic such as the total execution path length, PerfFuzz uses multi-dimensional feedback and independently maximizes execution counts for all program locations. This enables PerfFuzz to (1) find a variety of inputs that exercise distinct hot spots in a program and (2) generate inputs with higher total execution path length than previous approaches by escaping local maxima. PerfFuzz is also effective at generating inputs that demonstrate algorithmic complexity vulnerabilities. We implement PerfFuzz on top of AFL, a popular coverage-guided fuzzing tool, and evaluate PerfFuzz on four real-world C programs typically used in the fuzzing literature. We find that PerfFuzz outperforms prior work by generating inputs that exercise the most-hit program branch 5x to 69x times more, and result in 1.9x to 24.7x longer total execution paths.},
  booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
  pages     = {254–265},
  numpages  = {12},
  keywords  = {fuzz testing, algorithmic complexity, worst-case, performance},
  location  = {Amsterdam, Netherlands},
  series    = {ISSTA 2018}
}

@inproceedings{RFUZZ,
  author    = {Laeufer, Kevin and Koenig, Jack and Kim, Donggyu and Bachrach, Jonathan and Sen, Koushik},
  booktitle = {2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD)},
  title     = {RFUZZ: Coverage-Directed Fuzz Testing of RTL on FPGAs},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {1-8},
  doi       = {10.1145/3240765.3240842}
}

@inproceedings{RLCheck,
  author    = {Reddy, Sameer and Lemieux, Caroline and Padhye, Rohan and Sen, Koushik},
  title     = {Quickly Generating Diverse Valid Test Inputs with Reinforcement Learning},
  year      = {2020},
  isbn      = {9781450371216},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3377811.3380399},
  doi       = {10.1145/3377811.3380399},
  abstract  = {Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a black-box approach for generating valid test inputs. We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.},
  booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
  pages     = {1410–1421},
  numpages  = {12},
  location  = {Seoul, South Korea},
  series    = {ICSE '20}
}

@inproceedings{QuickSampler,
  author    = {Dutra, Rafael and Laeufer, Kevin and Bachrach, Jonathan and Sen, Koushik},
  title     = {Efficient Sampling of SAT Solutions for Testing},
  year      = {2018},
  isbn      = {9781450356381},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3180155.3180248},
  doi       = {10.1145/3180155.3180248},
  abstract  = {In software and hardware testing, generating multiple inputs which satisfy a given set of constraints is an important problem with applications in fuzz testing and stimulus generation. However, it is a challenge to perform the sampling efficiently, while generating a diverse set of inputs which satisfy the constraints. We developed a new algorithm QuickSampler which requires a small number of solver calls to produce millions of samples which satisfy the constraints with high probability. We evaluate QuickSampler on large real-world benchmarks and show that it can produce unique valid solutions orders of magnitude faster than other state-of-the-art sampling tools, with a distribution which is reasonably close to uniform in practice.},
  booktitle = {Proceedings of the 40th International Conference on Software Engineering},
  pages     = {549–559},
  numpages  = {11},
  keywords  = {sampling, constraint-based testing, stimulus generation, constrained-random verification},
  location  = {Gothenburg, Sweden},
  series    = {ICSE '18}
}

@inproceedings{PARTEMU,
  author    = {Lee Harrison and Hayawardh Vijayakumar and Rohan Padhye and Koushik Sen and Michael Grace},
  title     = {{PARTEMU}: Enabling Dynamic Analysis of {Real-World} {TrustZone} Software Using Emulation},
  booktitle = {29th USENIX Security Symposium (USENIX Security 20)},
  year      = {2020},
  isbn      = {978-1-939133-17-5},
  pages     = {789--806},
  url       = {https://www.usenix.org/conference/usenixsecurity20/presentation/harrison},
  publisher = {USENIX Association},
  month     = aug
}


@article{JavaScript2,
  author     = {Andreasen, Esben and Gong, Liang and M\o{}ller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
  title      = {A Survey of Dynamic Analysis and Test Generation for JavaScript},
  year       = {2017},
  issue_date = {September 2018},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {50},
  number     = {5},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3106739},
  doi        = {10.1145/3106739},
  abstract   = {JavaScript has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make JavaScript programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for JavaScript targeted at improving the correctness, reliability, performance, security, and privacy of JavaScript-based software.},
  journal    = {ACM Comput. Surv.},
  month      = {sep},
  articleno  = {66},
  numpages   = {36},
  keywords   = {test generation, dynamic languages, Program analysis}
}