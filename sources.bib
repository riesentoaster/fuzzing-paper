@article{UNIX,
  author     = {Miller, Barton P. and Fredriksen, Lars and So, Bryan},
  title      = {An Empirical Study of the Reliability of UNIX Utilities},
  year       = {1990},
  issue_date = {Dec. 1990},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {33},
  number     = {12},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/96267.96279},
  doi        = {10.1145/96267.96279},
  abstract   = {The following section describes the tools we built to test the utilities. These tools include the fuzz (random character) generator, ptyjig (to test interactive utilities), and scripts to automate the testing process. Next, we will describe the tests we performed, giving the types of input we presented to the utilities. Results from the tests will follow along with an analysis of the results, including identification and classification of the program bugs that caused the crashes. The final section presents concluding remarks, including suggestions for avoiding the types of problems detected by our study and some commentary on the bugs we found. We include an Appendix with the user manual pages for fuzz and ptyjig.},
  journal    = {Commun. ACM},
  month      = {dec},
  pages      = {32–44},
  numpages   = {13}
}

@inproceedings{DART,
  author    = {Godefroid, Patrice and Klarlund, Nils and Sen, Koushik},
  title     = {DART: Directed Automated Random Testing},
  year      = {2005},
  isbn      = {1595930566},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1065010.1065036},
  doi       = {10.1145/1065010.1065036},
  abstract  = {We present a new tool, named DART, for automatically testing software that combines three main techniques: (1) automated extraction of the interface of a program with its external environment using static source-code parsing; (2) automatic generation of a test driver for this interface that performs random testing to simulate the most general environment the program can operate in; and (3) dynamic analysis of how the program behaves under random testing and automatic generation of new test inputs to direct systematically the execution along alternative program paths. Together, these three techniques constitute Directed Automated Random Testing, or DART for short. The main strength of DART is thus that testing can be performed completely automatically on any program that compiles -- there is no need to write any test driver or harness code. During testing, DART detects standard errors such as program crashes, assertion violations, and non-termination. Preliminary experiments to unit test several examples of C programs are very encouraging.},
  booktitle = {Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {213–223},
  numpages  = {11},
  keywords  = {software testing, interfaces, random testing, automated test generation, program verification},
  location  = {Chicago, IL, USA},
  series    = {PLDI '05}
}

@inproceedings{SAGE,
  title     = {Automated Whitebox Fuzz Testing},
  author    = {Patrice Godefroid and Michael Y. Levin and David A. Molnar},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2008},
  url       = {https://api.semanticscholar.org/CorpusID:1296783}
}

@inproceedings{GrammarBasedWhiteboxFuzzing,
  author    = {Godefroid, Patrice and Kiezun, Adam and Levin, Michael Y.},
  title     = {Grammar-Based Whitebox Fuzzing},
  year      = {2008},
  isbn      = {9781595938602},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/1375581.1375607},
  doi       = {10.1145/1375581.1375607},
  abstract  = {Whitebox fuzzing is a form of automatic dynamic test generation, based on symbolic execution and constraint solving, designed for security testing of large applications. Unfortunately, the current effectiveness of whitebox fuzzing is limited when testing applications with highly-structured inputs, such as compilers and interpreters. These applications process their inputs in stages, such as lexing, parsing and evaluation. Due to the enormous number of control paths in early processing stages, whitebox fuzzing rarely reaches parts of the application beyond those first stages.In this paper, we study how to enhance whitebox fuzzing of complex structured-input applications with a grammar-based specification of their valid inputs. We present a novel dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver. We have implemented this algorithm and evaluated it on a large security-critical application, the JavaScript interpreter of Internet Explorer 7 (IE7). Results of our experiments show that grammar-based whitebox fuzzing explores deeper program paths and avoids dead-ends due to non-parsable inputs. Compared to regular whitebox fuzzing, grammar-based whitebox fuzzing increased coverage of the code generation module of the IE7 JavaScript interpreter from 53\% to 81\% while using three times fewer tests.},
  booktitle = {Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation},
  pages     = {206–215},
  numpages  = {10},
  keywords  = {grammars, automatic test generation, software testing, program verification},
  location  = {Tucson, AZ, USA},
  series    = {PLDI '08}
}

@inproceedings{KLEE,
  author    = {Cadar, Cristian and Dunbar, Daniel and Engler, Dawson},
  title     = {KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex Systems Programs},
  year      = {2008},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {We present a new symbolic execution tool, KLEE, capable of automatically generating tests that achieve high coverage on a diverse set of complex and environmentally-intensive programs. We used KLEE to thoroughly check all 89 stand-alone programs in the GNU COREUTILS utility suite, which form the core user-level environment installed on millions of Unix systems, and arguably are the single most heavily tested set of open-source programs in existence. KLEE-generated tests achieve high line coverage -- on average over 90\% per tool (median: over 94\%) -- and significantly beat the coverage of the developers' own hand-written test suite. When we did the same for 75 equivalent tools in the BUSYBOX embedded system suite, results were even better, including 100\% coverage on 31 of them.We also used KLEE as a bug finding tool, applying it to 452 applications (over 430K total lines of code), where it found 56 serious bugs, including three in COREUTILS that had been missed for over 15 years. Finally, we used KLEE to crosscheck purportedly identical BUSYBOX and COREUTILS utilities, finding functional correctness errors and a myriad of inconsistencies.},
  booktitle = {Proceedings of the 8th USENIX Conference on Operating Systems Design and Implementation},
  pages     = {209–224},
  numpages  = {16},
  location  = {San Diego, California},
  series    = {OSDI'08}
}

@article{Science,
  title    = {A systematic review of fuzzing techniques},
  journal  = {Computers \& Security},
  volume   = {75},
  pages    = {118-137},
  year     = {2018},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2018.02.002},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404818300658},
  author   = {Chen Chen and Baojiang Cui and Jinxin Ma and Runpu Wu and Jianchao Guo and Wenqian Liu},
  keywords = {Software bug, Vulnerability, Fuzzing, Dynamic symbolic execution, Coverage guide, Grammar representation, Scheduling algorithms, Taint analysis, Static analysis},
  abstract = {Fuzzing is an effective and widely used technique for finding security bugs and vulnerabilities in software. It inputs irregular test data into a target program to try to trigger a vulnerable condition in the program execution. Since the first random fuzzing system was constructed, fuzzing efficiency has been greatly improved by combination with several useful techniques, including dynamic symbolic execution, coverage guide, grammar representation, scheduling algorithms, dynamic taint analysis, static analysis and machine learning. In this paper, we will systematically review these techniques and their corresponding representative fuzzing systems. By introducing the principles, advantages and disadvantages of these techniques, we hope to provide researchers with a systematic and deeper understanding of fuzzing techniques and provide some references for this field.}
}

@inproceedings{Driller,
  title     = {Driller: Augmenting Fuzzing Through Selective Symbolic Execution},
  author    = {Nick Stephens and John Grosen and Christopher Salls and Andrew Dutcher and Ruoyu Wang and Jacopo Corbetta and Yan Shoshitaishvili and Christopher Kr{\"u}gel and Giovanni Vigna},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2016},
  url       = {https://api.semanticscholar.org/CorpusID:2388545}
}

@inproceedings{AFLPlusPlus,
  author    = {Andrea Fioraldi and Dominik Maier and Heiko Ei{\ss}feldt and Marc Heuse},
  title     = {{AFL++} : Combining Incremental Steps of Fuzzing Research},
  booktitle = {14th USENIX Workshop on Offensive Technologies (WOOT 20)},
  year      = {2020},
  url       = {https://www.usenix.org/conference/woot20/presentation/fioraldi},
  publisher = {USENIX Association},
  month     = aug
}

@inproceedings{AFLGo,
  author    = {B\"{o}hme, Marcel and Pham, Van-Thuan and Nguyen, Manh-Dung and Roychoudhury, Abhik},
  title     = {Directed Greybox Fuzzing},
  year      = {2017},
  isbn      = {9781450349468},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3133956.3134020},
  doi       = {10.1145/3133956.3134020},
  abstract  = {Existing Greybox Fuzzers (GF) cannot be effectively directed, for instance, towards problematic changes or patches, towards critical system calls or dangerous locations, or towards functions in the stack-trace of a reported vulnerability that we wish to reproduce. In this paper, we introduce Directed Greybox Fuzzing (DGF) which generates inputs with the objective of reaching a given set of target program locations efficiently. We develop and evaluate a simulated annealing-based power schedule that gradually assigns more energy to seeds that are closer to the target locations while reducing energy for seeds that are further away. Experiments with our implementation AFLGo demonstrate that DGF outperforms both directed symbolic-execution-based whitebox fuzzing and undirected greybox fuzzing. We show applications of DGF to patch testing and crash reproduction, and discuss the integration of AFLGo into Google's continuous fuzzing platform OSS-Fuzz. Due to its directedness, AFLGo could find 39 bugs in several well-fuzzed, security-critical projects like LibXML2. 17 CVEs were assigned.},
  booktitle = {Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2329–2344},
  numpages  = {16},
  keywords  = {reachability, directed testing, coverage-based greybox fuzzing, verifying true positives, patch testing, crash reproduction},
  location  = {Dallas, Texas, USA},
  series    = {CCS '17}
}
@inproceedings{Munch,
  author    = {Ognawala, Saahil and Hutzelmann, Thomas and Psallida, Eirini and Pretschner, Alexander},
  title     = {Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach},
  year      = {2018},
  isbn      = {9781450351911},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3167132.3167289},
  doi       = {10.1145/3167132.3167289},
  abstract  = {Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open-source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage.},
  booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
  pages     = {1475–1482},
  numpages  = {8},
  keywords  = {symbolic execution, function coverage, compositional analysis, software testing, fuzzing},
  location  = {Pau, France},
  series    = {SAC '18}
}

@article{Magma,
  author     = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
  title      = {Magma: A Ground-Truth Fuzzing Benchmark},
  year       = {2021},
  issue_date = {December 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {4},
  number     = {3},
  url        = {https://doi.org/10.1145/3428334},
  doi        = {10.1145/3428334},
  abstract   = {High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count---perhaps the most commonly-used performance metric---is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.},
  journal    = {Proc. ACM Meas. Anal. Comput. Syst.},
  month      = {jun},
  articleno  = {49},
  numpages   = {29},
  keywords   = {benchmark, fuzzing, performance evaluation, software security}
}

@inproceedings{TFuzz,
  author    = {Peng, Hui and Shoshitaishvili, Yan and Payer, Mathias},
  booktitle = {2018 IEEE Symposium on Security and Privacy (SP)},
  title     = {T-Fuzz: Fuzzing by Program Transformation},
  year      = {2018},
  volume    = {},
  number    = {},
  pages     = {697-710},
  doi       = {10.1109/SP.2018.00056}
}

@inproceedings{LearnFuzz,
  author    = {Godefroid, Patrice and Peleg, Hila and Singh, Rishabh},
  booktitle = {2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)},
  title     = {Learn\&Fuzz: Machine learning for input fuzzing},
  year      = {2017},
  volume    = {},
  number    = {},
  pages     = {50-59},
  doi       = {10.1109/ASE.2017.8115618}
}

@article{FuzzingASurvey,
  title   = {Fuzzing: a survey},
  author  = {Jun Li and Bodong Zhao and Chao Zhang},
  journal = {Cybersecurity},
  year    = {2018},
  volume  = {1},
  pages   = {1-13},
  url     = {https://api.semanticscholar.org/CorpusID:46928493}
}

@article{FuzzingASurveyforRoadmap,
  author     = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
  title      = {Fuzzing: A Survey for Roadmap},
  year       = {2022},
  issue_date = {January 2022},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {54},
  number     = {11s},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3512345},
  doi        = {10.1145/3512345},
  abstract   = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this article, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
  journal    = {ACM Comput. Surv.},
  month      = {sep},
  articleno  = {230},
  numpages   = {36},
  keywords   = {fuzzing theory, security, input space, Fuzz testing, automation}
}

@article{ArtScienceEngineeringFuzzing,
  author  = {Manès, Valentin J.M. and Han, HyungSeok and Han, Choongwoo and Cha, Sang Kil and Egele, Manuel and Schwartz, Edward J. and Woo, Maverick},
  journal = {IEEE Transactions on Software Engineering},
  title   = {The Art, Science, and Engineering of Fuzzing: A Survey},
  year    = {2021},
  volume  = {47},
  number  = {11},
  pages   = {2312-2331},
  doi     = {10.1109/TSE.2019.2946563}
}

@article{Demystifying,
  author     = {Mallissery, Sanoop and Wu, Yu-Sung},
  title      = {Demystify the Fuzzing Methods: A Comprehensive Survey},
  year       = {2023},
  issue_date = {March 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {3},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3623375},
  doi        = {10.1145/3623375},
  abstract   = {Massive software applications possess complex data structures or parse complex data structures; in such cases, vulnerabilities in the software become inevitable. The vulnerabilities are the source of cyber-security threats, and discovering this before the software deployment is challenging. Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. Each technique is wrapped beneath the black-, white-, and grey-box fuzzers to uncover diverse vulnerabilities. It consists of methods such as identifying structural information about the test cases to detect security vulnerabilities, symbolic and concrete program states to explore the unexplored locations, and full semantics of code coverage to create new test cases. We methodically examine each kind of fuzzers and contemporary fuzzers with a profound observation that addresses various research questions and systematically reviews and analyze the gaps and their solutions. Our survey comprised the recent related works on fuzzing techniques to demystify the fuzzing methods concerning the application domains and the target that, in turn, achieves higher code coverage and sound vulnerability detection.},
  journal    = {ACM Comput. Surv.},
  month      = {oct},
  articleno  = {71},
  numpages   = {38},
  keywords   = {vulnerability discovery, Automated testing, fuzzing, code inspection}
}

@article{FuzzingVulnerabilityDiscoveryTechniques,
  title    = {Fuzzing vulnerability discovery techniques: Survey, challenges and future directions},
  journal  = {Computers \& Security},
  volume   = {120},
  pages    = {102813},
  year     = {2022},
  issn     = {0167-4048},
  doi      = {https://doi.org/10.1016/j.cose.2022.102813},
  url      = {https://www.sciencedirect.com/science/article/pii/S0167404822002073},
  author   = {Craig Beaman and Michael Redbourne and J. Darren Mummery and Saqib Hakak},
  keywords = {Vulnerability, Fuzzing, Software Security, Fuzzers, Software Vulnerability, Vulnerability accessment, Static code analysis, Security},
  abstract = {Fuzzing is a powerful tool for vulnerability discovery in software, with much progress being made in the field in recent years. There is limited literature available on the fuzzing vulnerability discovery approaches. Hence, in this paper, an attempt has been made to explore the recent advances in the area of fuzzing vulnerability discovery and to propose a refinement to the classification of fuzzers. Furthermore, we have identified key research challenges and potential future areas of research that might provide new insight to researchers.}
}

@inproceedings{TaintScope,
  author    = {Wang, Tielei and Wei, Tao and Gu, Guofei and Zou, Wei},
  booktitle = {2010 IEEE Symposium on Security and Privacy},
  title     = {TaintScope: A Checksum-Aware Directed Fuzzing Tool for Automatic Software Vulnerability Detection},
  year      = {2010},
  volume    = {},
  number    = {},
  pages     = {497-512},
  doi       = {10.1109/SP.2010.37}
}

@article{ReviewThreeDecades,
  author     = {Cadar, Cristian and Sen, Koushik},
  title      = {Symbolic Execution for Software Testing: Three Decades Later},
  year       = {2013},
  issue_date = {February 2013},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/2408776.2408795},
  doi        = {10.1145/2408776.2408795},
  abstract   = {The challenges---and great promise---of modern symbolic execution techniques, and the tools to help implement them.},
  journal    = {Commun. ACM},
  month      = {feb},
  pages      = {82–90},
  numpages   = {9}
}

@article{CUTE,
  author     = {Sen, Koushik and Marinov, Darko and Agha, Gul},
  title      = {CUTE: A Concolic Unit Testing Engine for C},
  year       = {2005},
  issue_date = {September 2005},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {30},
  number     = {5},
  issn       = {0163-5948},
  url        = {https://doi.org/10.1145/1095430.1081750},
  doi        = {10.1145/1095430.1081750},
  abstract   = {In unit testing, a program is decomposed into units which are collections of functions. A part of unit can be tested by generating inputs for a single entry function. The entry function may contain pointer arguments, in which case the inputs to the unit are memory graphs. The paper addresses the problem of automating unit testing with memory graphs as inputs. The approach used builds on previous work combining symbolic and concrete execution, and more specifically, using such a combination to generate test inputs to explore all feasible execution paths. The current work develops a method to represent and track constraints that capture the behavior of a symbolic execution of a unit with memory graphs as inputs. Moreover, an efficient constraint solver is proposed to facilitate incremental generation of such test inputs. Finally, CUTE, a tool implementing the method is described together with the results of applying CUTE to real-world examples of C code.},
  journal    = {SIGSOFT Softw. Eng. Notes},
  month      = {sep},
  pages      = {263–272},
  numpages   = {10},
  keywords   = {concolic testing, unit testing, random testing, testing C programs, data structure testing, explicit path model-checking}
}

@inproceedings{BuzzFuzz,
  author    = {Ganesh, Vijay and Leek, Tim and Rinard, Martin},
  booktitle = {2009 IEEE 31st International Conference on Software Engineering},
  title     = {Taint-based directed whitebox fuzzing},
  year      = {2009},
  volume    = {},
  number    = {},
  pages     = {474-484},
  doi       = {10.1109/ICSE.2009.5070546}
}

@article{SAGEImpact,
  author     = {Godefroid, Patrice and Levin, Michael Y. and Molnar, David},
  title      = {SAGE: Whitebox Fuzzing for Security Testing: SAGE Has Had a Remarkable Impact at Microsoft.},
  year       = {2012},
  issue_date = {January 2012},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {10},
  number     = {1},
  issn       = {1542-7730},
  url        = {https://doi.org/10.1145/2090147.2094081},
  doi        = {10.1145/2090147.2094081},
  abstract   = {Most ACM Queue readers might think of "program verification research" as mostly theoretical with little impact on the world at large. Think again. If you are reading these lines on a PC running some form of Windows (like 93-plus percent of PC users--that is, more than a billion people), then you have been affected by this line of work--without knowing it, which is precisely the way we want it to be.},
  journal    = {Queue},
  month      = {jan},
  pages      = {20–27},
  numpages   = {8}
}

@article{DowserArticle,
  title   = {Dowser: A Guided Fuzzer for Finding Buffer Overflow Vulnerabilities},
  author  = {Istv{\'a}n Haller and Asia Slowinska and Matthias Neugschwandtner and Herbert Bos},
  journal = {login Usenix Mag.},
  year    = {2013},
  volume  = {38},
  url     = {https://api.semanticscholar.org/CorpusID:56594822}
}

@inproceedings{Dowser,
  author    = {Haller, Istvan and Slowinska, Asia and Neugschwandtner, Matthias and Bos, Herbert},
  title     = {Dowsing for Overflows: A Guided Fuzzer to Find Buffer Boundary Violations},
  year      = {2013},
  isbn      = {9781931971034},
  publisher = {USENIX Association},
  address   = {USA},
  abstract  = {Dowser is a 'guided' fuzzer that combines taint tracking, program analysis and symbolic execution to find buffer overflow and underflow vulnerabilities buried deep in a program's logic. The key idea is that analysis of a program lets us pinpoint the right areas in the program code to probe and the appropriate inputs to do so.Intuitively, for typical buffer overflows, we need consider only the code that accesses an array in a loop, rather than all possible instructions in the program. After finding all such candidate sets of instructions, we rank them according to an estimation of how likely they are to contain interesting vulnerabilities. We then subject the most promising sets to further testing. Specifically, we first use taint analysis to determine which input bytes influence the array index and then execute the program symbolically, making only this set of inputs symbolic. By constantly steering the symbolic execution along branch outcomes most likely to lead to overflows, we were able to detect deep bugs in real programs (like the nginx webserver, the inspircd IRC server, and the ffmpeg videoplayer). Two of the bugs we found were previously undocumented buffer overflows in ffmpeg and the poppler PDF rendering library.},
  booktitle = {Proceedings of the 22nd USENIX Conference on Security},
  pages     = {49–64},
  numpages  = {16},
  location  = {Washington, D.C.},
  series    = {SEC'13}
}

@inproceedings{BORG,
  author    = {Neugschwandtner, Matthias and Milani Comparetti, Paolo and Haller, Istvan and Bos, Herbert},
  title     = {The BORG: Nanoprobing Binaries for Buffer Overreads},
  year      = {2015},
  isbn      = {9781450331913},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2699026.2699098},
  doi       = {10.1145/2699026.2699098},
  abstract  = {Automated program testing tools typically try to explore, and cover, as much of a tested program as possible, while attempting to trigger and detect bugs. An alternative and complementary approach can be to first select a specific part of a program that may be subject to a specific class of bug, and then narrowly focus exploration towards program paths that could trigger such a bug.In this work, we introduce the BORG (Buffer Over-Read Guard), a testing tool that uses static and dynamic program analysis, taint propagation and symbolic execution to detect buffer overread bugs in real-world programs. BORG works by first selecting buffer accesses that could lead to an overread and then guiding symbolic execution towards those accesses along program paths that could actually lead to an overread. BORG operates on binaries and does not require source code. To demonstrate BORG's effectiveness, we use it to detect overreads in six complex server applications and libraries, including lighttpd, FFmpeg and ClamAV.},
  booktitle = {Proceedings of the 5th ACM Conference on Data and Application Security and Privacy},
  pages     = {87–97},
  numpages  = {11},
  keywords  = {out-of-bounds access, symbolic execution guidance, dynamic symbolic execution, buffer overread, targeted testing},
  location  = {San Antonio, Texas, USA},
  series    = {CODASPY '15}
}


@inproceedings{MoWF,
  author    = {Pham, Van-Thuan and B\"{o}hme, Marcel and Roychoudhury, Abhik},
  title     = {Model-Based Whitebox Fuzzing for Program Binaries},
  year      = {2016},
  isbn      = {9781450338455},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/2970276.2970316},
  doi       = {10.1145/2970276.2970316},
  abstract  = {Many real-world programs take highly structured and very complex inputs. The automated testing of such programs is non-trivial. If the test input does not adhere to a specific file format, the program returns a parser error. For symbolic execution-based whitebox fuzzing the corresponding error handling code becomes a significant time sink. Too much time is spent in the parser exploring too many paths leading to trivial parser errors. Naturally, the time is better spent exploring the functional part of the program where failure with valid input exposes deep and real bugs in the program. In this paper, we suggest to leverage information about the file format and the data chunks of existing, valid files to swiftly carry the exploration beyond the parser code. We call our approach Model-based Whitebox Fuzzing (MoWF) because the file format input model of blackbox fuzzers can be exploited as a constraint on the vast input space to rule out most invalid inputs during path exploration in symbolic execution. We evaluate on 13 vulnerabilities in 8 large program binaries with 6 separate file formats and found that MoWF exposes all vulnerabilities while both, traditional whitebox fuzzing and model-based blackbox fuzzing, expose only less than half, respectively. Our experiments also demonstrate that MoWF exposes 70\% vulnerabilities without any seed inputs.},
  booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
  pages     = {543–553},
  numpages  = {11},
  keywords  = {Symbolic Execution, Program Binaries},
  location  = {Singapore, Singapore},
  series    = {ASE '16}
}

@article{S2E,
  author     = {Chipounov, Vitaly and Kuznetsov, Volodymyr and Candea, George},
  title      = {S2E: A Platform for in-Vivo Multi-Path Analysis of Software Systems},
  year       = {2011},
  issue_date = {March 2011},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {46},
  number     = {3},
  issn       = {0362-1340},
  url        = {https://doi.org/10.1145/1961296.1950396},
  doi        = {10.1145/1961296.1950396},
  abstract   = {This paper presents S2E, a platform for analyzing the properties and behavior of software systems. We demonstrate S2E's use in developing practical tools for comprehensive performance profiling, reverse engineering of proprietary software, and bug finding for both kernel-mode and user-mode binaries. Building these tools on top of S2E took less than 770 LOC and 40 person-hours each.S2E's novelty consists of its ability to scale to large real systems, such as a full Windows stack. S2E is based on two new ideas: selective symbolic execution, a way to automatically minimize the amount of code that has to be executed symbolically given a target analysis, and relaxed execution consistency models, a way to make principled performance/accuracy trade-offs in complex analyses. These techniques give S2E three key abilities: to simultaneously analyze entire families of execution paths, instead of just one execution at a time; to perform the analyses in-vivo within a real software stack--user programs, libraries, kernel, drivers, etc.--instead of using abstract models of these layers; and to operate directly on binaries, thus being able to analyze even proprietary software.Conceptually, S2E is an automated path explorer with modular path analyzers: the explorer drives the target system down all execution paths of interest, while analyzers check properties of each such path (e.g., to look for bugs) or simply collect information (e.g., count page faults). Desired paths can be specified in multiple ways, and S2E users can either combine existing analyzers to build a custom analysis tool, or write new analyzers using the S2E API.},
  journal    = {SIGPLAN Not.},
  month      = {mar},
  pages      = {265–278},
  numpages   = {14},
  keywords   = {dbt, virtualization, consistency models, testing, symbolic execution, binary, analysis, performance, in-vivo, framework}
}

@inproceedings{Mayhem,
  author    = {Cha, Sang Kil and Avgerinos, Thanassis and Rebert, Alexandre and Brumley, David},
  booktitle = {2012 IEEE Symposium on Security and Privacy},
  title     = {Unleashing Mayhem on Binary Code},
  year      = {2012},
  volume    = {},
  number    = {},
  pages     = {380-394},
  doi       = {10.1109/SP.2012.31}
}

@inproceedings{VUzzer,
  title     = {VUzzer: Application-aware Evolutionary Fuzzing},
  author    = {Sanjay Rawat and Vivek Jain and Ashish Kumar and Lucian Cojocar and Cristiano Giuffrida and Herbert Bos},
  booktitle = {Network and Distributed System Security Symposium},
  year      = {2017},
  url       = {https://api.semanticscholar.org/CorpusID:2354736}
}

@inproceedings{SYMFUZZ,
  author    = {Cha, Sang Kil and Woo, Maverick and Brumley, David},
  booktitle = {2015 IEEE Symposium on Security and Privacy},
  title     = {Program-Adaptive Mutational Fuzzing},
  year      = {2015},
  volume    = {},
  number    = {},
  pages     = {725-741},
  doi       = {10.1109/SP.2015.50}
}

@inproceedings{EvaluatingFuzzTesting,
  author    = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
  title     = {Evaluating Fuzz Testing},
  year      = {2018},
  isbn      = {9781450356930},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3243734.3243804},
  doi       = {10.1145/3243734.3243804},
  abstract  = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
  booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
  pages     = {2123–2138},
  numpages  = {16},
  keywords  = {fuzzing, evaluation, security},
  location  = {Toronto, Canada},
  series    = {CCS '18}
}

@article{HackArtScience,
  author     = {Godefroid, Patrice},
  title      = {Fuzzing: Hack, Art, and Science},
  year       = {2020},
  issue_date = {February 2020},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {63},
  number     = {2},
  issn       = {0001-0782},
  url        = {https://doi.org/10.1145/3363824},
  doi        = {10.1145/3363824},
  abstract   = {Reviewing software testing techniques for finding security vulnerabilities.},
  journal    = {Commun. ACM},
  month      = {jan},
  pages      = {70–76},
  numpages   = {7}
}

@article{FuzzingTheStateOfTheArt,
  title   = {Fuzzing: The State of the Art},
  journal = {DSTO Defence Science and Technology Organisation},
  author  = {Richard McNally and Kenneth Kwok-Hei Yiu and Duncan A. Grove and Damien Gerhardy},
  year    = {2012},
  url     = {https://api.semanticscholar.org/CorpusID:15447929}
}

@article{SystematicReview2023,
  title     = {A systematic review of fuzzing},
  author    = {Zhao, Xiaoqi and Qu, Haipeng and Xu, Jianliang and Li, Xiaohui and Lv, Wenjie and Wang, Gai-Ge},
  journal   = {Soft Computing},
  pages     = {1--30},
  year      = {2023},
  publisher = {Springer}
}

@article{IoT,
  author  = {Eceiza, Maialen and Flores, Jose Luis and Iturbe, Mikel},
  journal = {IEEE Internet of Things Journal},
  title   = {Fuzzing the Internet of Things: A Review on the Techniques and Challenges for Efficient Vulnerability Discovery in Embedded Systems},
  year    = {2021},
  volume  = {8},
  number  = {13},
  pages   = {10390-10411},
  doi     = {10.1109/JIOT.2021.3056179}
}

@article{ChallengesAndReflections,
  author  = {Boehme, Marcel and Cadar, Cristian and ROYCHOUDHURY, Abhik},
  journal = {IEEE Software},
  title   = {Fuzzing: Challenges and Reflections},
  year    = {2021},
  volume  = {38},
  number  = {3},
  pages   = {79-86},
  doi     = {10.1109/MS.2020.3016773}
}

@article{Network,
  author   = {Munea, Tewodros Legesse and Lim, Hyunwoo and Shon, Taeshik},
  title    = {Network protocol fuzz testing for information systems and applications: a survey and taxonomy},
  journal  = {Multimedia Tools and Applications},
  year     = {2016},
  month    = {Nov},
  day      = {01},
  volume   = {75},
  number   = {22},
  pages    = {14745-14757},
  abstract = {Fuzzing or fuzz testing has been introduced as a software testing technique to reduce vulnerabilities in software systems or given targets. To achieve a maximum benefit-to-cost ratio and without complication, we use fuzz testing [11]. In addition, during the development and debugging of a system, we may fail to notice the kinds of shortcoming that fuzz testing can expose. Fuzz testing types are different depending on the target they fuzz. Application, file format, and protocol fuzzing are the most common fuzzing types. A protocol fuzzer sends counterfeit packets to a target system while changing the normal packet en-route and sometimes replaying them. In addition, a protocol fuzzer sometimes acts as proxy server for clients. This survey study examines network protocol fuzz testing. We identified several studies on network protocol fuzzing. Most focus on application layers of the Open Systems Interconnection model. We primarily review the approaches of five studies and the targets and protocol layers they fuzz. We then develop criteria to compare these approaches in detail.},
  issn     = {1573-7721},
  doi      = {10.1007/s11042-015-2763-6},
  url      = {https://doi.org/10.1007/s11042-015-2763-6}
}

@article{FuzzingStateOfTheArt2018,
  author  = {Liang, Hongliang and Pei, Xiaoxiao and Jia, Xiaodong and Shen, Wuwei and Zhang, Jian},
  journal = {IEEE Transactions on Reliability},
  title   = {Fuzzing: State of the Art},
  year    = {2018},
  volume  = {67},
  number  = {3},
  pages   = {1199-1218},
  doi     = {10.1109/TR.2018.2834476}
}

@inproceedings{Firmware,
  author    = {Zhang, Chi and Wang, Yu and Wang, Linzhang},
  title     = {Firmware Fuzzing: The State of the Art},
  year      = {2021},
  isbn      = {9781450388191},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3457913.3457934},
  doi       = {10.1145/3457913.3457934},
  abstract  = {Background: Firmware is the enable software of Internet of Things (IoT) devices, and its software vulnerabilities are one of the primary reason of IoT devices being exploited. Due to the limited resources of IoT devices, it is impractical to deploy sophisticated run-time protection techniques. Under an insecure network environment, when a firmware is exploited, it may lead to denial of service, information disclosure, elevation of privilege, or even life-threatening. Therefore, firmware vulnerability detection by fuzzing has become the key to ensure the security of IoT devices, and has also become a hot topic in academic and industrial research. With the rapid growth of the existing IoT devices, the size and complexity of firmware, the variety of firmware types, and the firmware defects, existing IoT firmware fuzzing methods face challenges. Objective: This paper summarizes the typical types of IoT firmware fuzzing methods, analyzes the contribution of these works, and summarizes the shortcomings of existing fuzzing methods. Method: We design several research questions, extract keywords from the research questions, then use the keywords to search for related literature. Result: We divide the existing firmware fuzzing work into real-device-based fuzzing and simulation-based fuzzing according to the firmware execution environment, and simulation-based fuzzing is the mainstream in the future; we found that the main types of vulnerabilities targeted by existing fuzzing methods are memory corruption vulnerabilities; firmware fuzzing faces more difficulties than ordinary software fuzzing. Conclusion: Through the analysis of the advantages and disadvantages of different methods, this review provides guidance for further improving the performance of fuzzing techniques, and proposes several recommendations from the findings of this review.},
  booktitle = {Proceedings of the 12th Asia-Pacific Symposium on Internetware},
  pages     = {110–115},
  numpages  = {6},
  keywords  = {literature review, firmware, Internet of Things, fuzzing},
  location  = {Singapore, Singapore},
  series    = {Internetware '20}
}

@article{Embedded,
  author     = {Yun, Joobeom and Rustamov, Fayozbek and Kim, Juhwan and Shin, Youngjoo},
  title      = {Fuzzing of Embedded Systems: A Survey},
  year       = {2022},
  issue_date = {July 2023},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {55},
  number     = {7},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3538644},
  doi        = {10.1145/3538644},
  abstract   = {Security attacks abuse software vulnerabilities of IoT devices; hence, detecting and eliminating these vulnerabilities immediately are crucial. Fuzzing is an efficient method to identify vulnerabilities automatically, and many publications have been released to date. However, fuzzing for embedded systems has not been studied extensively owing to various obstacles, such as multi-architecture support, crash detection difficulties, and limited resources. Thus, the article introduces fuzzing techniques for embedded systems and the fuzzing differences for desktop and embedded systems. Further, we collect state-of-the-art technologies, discuss their advantages and disadvantages, and classify embedded system fuzzing tools. Finally, future directions for fuzzing research of embedded systems are predicted and discussed.},
  journal    = {ACM Comput. Surv.},
  month      = {dec},
  articleno  = {137},
  numpages   = {33},
  keywords   = {embedded systems, fuzzing, software testing, Firmware fuzzing, symbolic execution, concolic execution, IoT devices, firmware analysis}
}

@inproceedings{JavaScript,
  author    = {Tian, Ye and Qin, Xiaojun and Gan, Shuitao},
  title     = {Research on Fuzzing Technology for JavaScript Engines},
  year      = {2021},
  isbn      = {9781450389853},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3487075.3487107},
  doi       = {10.1145/3487075.3487107},
  abstract  = {JavaScript engine is the core component of web browsers, whose security issues are one of the critical aspects of the overall Web Eco-Security. Fuzzing technology, as an efficient software testing approach, has been widely applied to detecting vulnerabilities in different JavaScript engines, which is a security research hotspot at present. Based on systematical dissection of existing fuzzing methods, this paper reviews the development and technical ideas of JavaScript Engine Fuzzing combined with taxonomy, proposes a general framework of JavaScript Engine Fuzzing and analyzes the key techniques involved. Finally, we discuss the core issues that restrict efficiency in current research and present an outlook on the future trends of JavaScript Engine Fuzzing.},
  booktitle = {Proceedings of the 5th International Conference on Computer Science and Application Engineering},
  articleno = {32},
  numpages  = {7},
  keywords  = {Fuzzing, Browser security, JavaScript engine, Vulnerability detection},
  location  = {Sanya, China},
  series    = {CSAE '21}
}
@article{SearchStrategies,
  title    = {A Systematic Review of Search Strategies in Dynamic Symbolic Execution},
  journal  = {Computer Standards \& Interfaces},
  volume   = {72},
  pages    = {103444},
  year     = {2020},
  issn     = {0920-5489},
  doi      = {https://doi.org/10.1016/j.csi.2020.103444},
  url      = {https://www.sciencedirect.com/science/article/pii/S0920548919300066},
  author   = {Arash Sabbaghi and Mohammad Reza Keyvanpour},
  keywords = {Software Quality, Software Testing, Dynamic Symbolic Execution, Concolic Testing, Execution Generated Testing, Search Strategy},
  abstract = {One of the major concerns of dynamic symbolic execution (DSE) based automated test case generation is its huge search space which restricts its usage for industrial-size program testing. In fact, DSE performs test case generation by exploring paths of the program, and the number of program paths is exponential in the number of branch conditions encountered during execution. Thus, by increasing the number of branches, the search space will be extremely large and without applying an effective and efficient technique to explore the search space, DSE would fail to achieve the predesignated goals in the given budget. To this end, different search strategies have been proposed to prioritize program paths and to select the most promising ones with respect to the testing goal. In this paper, we conduct a comprehensive systematic review of search strategies in DSE. We collect different techniques and methods concerning the topic, classify and summarize them, highlight their advantages and drawbacks, and provide a complete comparison of the methods in each category. The classification is carried out according to the type of search and also the information source exploited by the strategies to direct DSE. We also analyze the evaluation methodologies of experiments reported on this subject, give a general overview of them, perform a set of experiments and provide a set of guidelines for conducting future experiments in this area of research.}
}