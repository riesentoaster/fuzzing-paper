\documentclass[12pt]{article}
\usepackage{hyperref}
\usepackage{csquotes}


\title{Symbolic Execution in Fuzzing — A Survey}
\author{Valentin Huber}
\begin{document}
\include{settings}

\maketitle
\tableofcontents

\section{Related Work}
\begin{itemize}
    \item A systematic review of fuzzing techniques (2018)\cite{Science}
    \item Fuzzing: A Survey (2018)\cite{FuzzingASurvey}
    \item The Art, Science, and Engineering of Fuzzing: A Survey (2021)\cite{ArtScienceEngineeringFuzzing}
    \item Fuzzing: A Survey for Roadmap (2022)\cite{FuzzingASurveyforRoadmap}
    \item Fuzzing vulnerability discovery techniques: Survey, challenges and future directions (2022)\cite{FuzzingVulnerabilityDiscoveryTechniques}
    \item Demystify the Fuzzing Methods: A Comprehensive Survey (2023)\cite{Demystifying}
\end{itemize}


\section{Random notes}
\begin{itemize}
    \item \textquote{Today, testing is the primary way to check the correctness of software. Billions of dollars are spent on testing in the software industry, as testing usually accounts for about 50\% of the cost of software development. It was recently estimated that software failures currently cost the US economy alone about \$60 billion every year, and that improvements in software testing infrastructure might save one-third of this cost.}\cite{DART}
    \item \textquote{The blackbox and whitebox strategies achieved similar results in all categories. This shows that, when testing applications with highly-structured inputs in a limited amount of time (2 hours), whitebox fuzzing, with the power of symbolic execution, does not improve much over sim- ple blackbox fuzzing. In fact, in the code generator, those grammar-less strategies do not improve coverage much above the initial set of seed inputs.}\cite{GrammarBasedWhiteboxFuzzing}
\end{itemize}

\section{Fuzzing types}
\subsection{Random input generation}
\subsubsection{An Empirical Study of the Reliability of UNIX Utilities (1990)}
\begin{itemize}
    \item \cite{UNIX}
    \item OG Fuzzing paper
    \item Started because in a stormy night, electrical interference on a dial-up connection
    \item Authors were surprised by amount of crashes, and artificially produced those.
    \item Generates random data (all chars/only printable chars, with or without NULL), throws them against a program
    \item Were able to crash or hang between 24 and 33\% of programs on different UNIX systems
    \item Different error categories: pointer and array errors, unchecked return codes, input functions, sub-processes, interaction effects, bad error handling, signed characters, race conditions and undetermined.
\end{itemize}

\subsection{Symbolic Execution}
\subsubsection{DART (2005)}
\begin{itemize}
    \item \cite{DART}
    \item Automated extraction of interface and env based on static source-code parsing
    \item Starts with random input, then uses symbex (without calling it symbex) to choose a different path
    \item Introduces a lot of concepts that I understand to be base level for symbex
    \item Has a unclear distinction to symbex, argues that symbex is stuck at expressions that aren't an issue with the symbex I know
    \item Concolic execution, fallback on concrete value whenever stuck
    \item Works on C code
    \item Positioned against static code analysis, which produces a lot of false positives while errors reported by DART are \textquote{trivially sound}\cite{DART}
    \item Run on a Pentium III 800MHz
    \item \textquote{As illustrated by the examples in Section 2, DART is able to alleviate some of the limitations of symbolic execution by exploiting dynamic information obtained from a concrete execution matching the symbolic constraints, by using dynamic test generation, and by instrumenting the program to check whether the input values generated next have the expected effect on the program.}\cite{DART}
\end{itemize}

\subsubsection{SAGE (2008)}
\begin{itemize}
    \item \cite{SAGE}
    \item First Whitebox Fuzzing paper so far.
    \item Developed at Microsoft.
    \item Does minor optimization to be able to perform partial symbex
    \item New invention: "Generational Search" — flips every branching condition after a symbex run to test in the next run, thus requiring fewer symbex runs overall.
    \item Uses concolic symbex whenever it gets too complex (i.e. interaction with the environment). It then checks whether the expected execution path is actually chosen and if not recovers (so-called "divergence").
    \item Runs on x86, Windows, file-reading applications.
    \item Found some vulnerabilities in media parsing engines and Office 2007.
    \item Further findings: symbex is slow (duh), at least two orders of magnitude compared to concrete execution.
    \item Divergences are common (60\% of runs). This is because a lot of instructions were concretized to help with performance.
    \item No clear correlation between coverage and crashes, only weak effect when using a block coverage based heuristic to choose next execution.
    \item tl;dr: Runs concolic symbex, records run, flips every branch condition on its own, and solves the constraint formulas to generate inputs that choose a different path at each branch.
    \item Struggles with highly structured input like compilers and interpreters. Issue: \textquote{Due to the enormous number of control paths in early process- ing stages, whitebox fuzzing rarely reaches parts of the application beyond these first stages.}\cite{GrammarBasedWhiteboxFuzzing}.
    \item Also: Parsers sometimes use hash functions to match tokens, which make symbex impossible because they cannot be inverted.\cite{GrammarBasedWhiteboxFuzzing}.
\end{itemize}

\subsubsection{KLEE (2008)}
\begin{itemize}
    \item \cite{KLEE}
    \item Wide array of tests including GNU COREUTILS, BUSYBOX, MINIX, and HISTAR (430K LOC, 452 programs)
    \item Tests programs and OS Kernel (HISTAR)
    \item Found multiple high-profile errors (ten fatals in COREUTILS, three older than 15 years)
    \item Compares functionality of different implementations of the same specs
    \item Checks each error on the real binary, so no false positives theoretically (but because non-determinism and bugs in KLEE there are some in practice)
    \item Works on LLVM basis (so not binary, doesn't work for projects where source code is unavailable)
    \item Extensive env modelling, including command line args, files, file metadata, env variables, failing system calls
    \item Path explosion combated with copy-on-write in state
    \item Performs query optimization (expression rewriting like mathematical simplifications, and using more efficient operations), constraint set simplification, constraint independence and a counter-example cache
    \item Alternates between random and coverage-optimized choice of next branch to execute
    \item New development: Better env modelling (not just dropping back on concrete values)
\end{itemize}

\subsubsection{Grammar-based Whitebox Fuzzing (2008)}
\begin{itemize}
    \item \cite{GrammarBasedWhiteboxFuzzing}
    \item Follow-up to SAGE\cite{SAGE}
    \item SAGE struggled with highly structured inputs. Which is where this paper comes in.
    \item \textquote{We present a dynamic test generation algorithm where symbolic execution directly generates grammar-based constraints whose satisfiability is checked using a custom grammar-based constraint solver.}\cite{GrammarBasedWhiteboxFuzzing}
    \item Two main parts:
          \begin{enumerate}
              \item \textquote{Generation of higher-level symbolic constraints, expressed in terms of symbolic grammar tokens returned by the lexer, instead of the traditional symbolic bytes read as input.}\cite{GrammarBasedWhiteboxFuzzing}
              \item \textquote{A custom constraint solver that solves constraints on symbolic grammar tokens. The solver looks for solutions that satisfy the constraints and are accepted by a given (context-free) grammar.}\cite{GrammarBasedWhiteboxFuzzing}
          \end{enumerate}
    \item Basically wrote their own custom token-based (as opposed to bit/byte-based) symbex engine.
    \item Does not mark input bytes as symbolic, but the tokens returned by the tokenization function in the parser, implemented based on SAGE\cite{SAGE}
          \begin{itemize}
              \item Also tries to only do this without using a grammar, so symbex based on tokens without pruning invalid inputs.
          \end{itemize}
    \item When negating constraints allows to generate input that will be parsed (does not use \textit{any} byte, but one that will conform to the manually provided context-free grammar).
    \item Also allows to quickly prove that flipping certain conditions isn't possible (while still conforming to the grammar) without even running the code.
    \item If the parser has more constraints than the context-free grammar provided (like basic type checks or, e. g. in network protocols, the number $k$ followed by $k$ records, which cannot be represented as context-free grammar), this makes the system less efficient, but the outputs are still complete.
    \item Requires no source modifications
    \item \textquote{We use the official JavaScript grammar. The grammar is quite large: 189 productions, 82 terminals (tokens), and 102 nonterminals.}\cite{GrammarBasedWhiteboxFuzzing}
    \item Downside: Requires some domain knowledge:\begin{itemize}
              \item Formal grammar structure (available for many input formats)
              \item Identifying the tokenization function in the parser that needs to be instrumented (apparently usually fairly straight-forward, by looking for functions with names that contain \textit{token, nextToken, scan} or something similar)
              \item Creating a de-tokenization function to generate input byte strings from input token strings generated by a context-free constraint solver.
          \end{itemize}
    \item This system doesn't check the lexer and parser for bugs, but one can just use traditional whitebox fuzzing (they say that coverage is similar to other approaches, but will likely not cover the error handling as well)
    \item Tested on IE7s JS engine
\end{itemize}

\section{TODOs}
\subsection{Related}
\begin{itemize}
    \item Grammar-based Whitebox Fuzzing\cite{GrammarBasedWhiteboxFuzzing} (follow-up to SAGE\cite{SAGE})
    \item AFLGo (Directed Greybox Fuzzing)\cite{AFLGo} (follow-up to Grammar-based Whitebox Fuzzing I think)
\end{itemize}

\subsection{New}
\begin{itemize}
    \item AFL++\cite{AFLPlusPlus}
    \item Driller: Augmenting Fuzzing Through Selective Symbolic Execution\cite{Driller}
    \item Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach\cite{Munch}
    \item Magma: A Ground-Truth Fuzzing Benchmark\cite{Magma}
    \item T-Fuzz: fuzzing by program transformation\cite{TFuzz}
    \item Learn\&Fuzz: Machine Learning for Input Fuzzing\cite{LearnFuzz}
\end{itemize}

\bibliography{sources}
\bibliographystyle{ieeetr}

\end{document}
